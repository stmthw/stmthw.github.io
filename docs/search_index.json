[["index.html", "Data Portfolio Chapter 1 README 1.1 Structure 1.2 Why Bookdown?", " Data Portfolio Matthew Robert Sears 2022-10-29 Chapter 1 README This is a selection of my projects &#x2013; curricular, extracurricular, and personal &#x2013; knitted together as a portfolio using bookdown (Xie 2022) to showcase my skill set in the realm of data and mathematics. Each chapter is a project with the most recent one at the top. Project identifiers are &#xA0;[subject | content]: [ML, DA | R, PY, MATH] The two main subjects are machine learning and data analysis, written in either R or python, incorporating math, SQL, or shell scripts. SQL and ZSH aren&#x2019;t included as identifiers but are shown when used. 1.1 Structure Project 1: NASA-SVM &#xA0; [ ML | PY ] Acronym disambiguation with machine learning using NASA white paper abstracts Project 2: Google Capstone &#xA0; [ DA | R ] Data analysis capstone project using Chicago cyclist metadata Project 3: Visualizing Sentiment &#xA0; [ DA | R ] Database querying with SQL and visualizing tweet sentiments Project 4: NN-XOR &#xA0; [ ML | PY , MATH ] The mathematics behind backpropagation and learning XOR, with a Tensorflow example Project 5: NN-MNIST &#xA0; [ ML | R , MATH ] Building and training a neural net from scratch using math and MNIST handwritten digits 1.2 Why Bookdown? Showcases code and provides transparency of the &#x201C;tools&#x201D; I use (and in times create, e.g., shell scripts or helper functions) &#x201C;Proof it works&#x201D; notebooks (all of the code on here ran successfully) Organizes and cleanly displays everything with a minimalist theme It&#x2019;s an amalgam of my interests and aligns with my professional skill set I already use bookdown when writing books because I love markdown I personally use obsidian for all of my notes References "],["nasa-svm.html", "Chapter 2 NASA SVM [ML|PY] 2.1 Description 2.2 Algorithm 2.3 Analysis 2.4 Conclusions 2.5 Future Work? Source Code", " Chapter 2 NASA SVM [ML|PY] Acronym disambiguation with machine learning A personal project using python and a public NASA dataset related to acronyms in public NASA white paper abstracts. My source of motivation comes from the suggestion in the dataset description: This was found to be a suitable dataset for training disambiguation models that use the context of the surrounding sentences to predict the correct meaning of the acronym. The prototype machine-learning models created from this dataset have not been released. So I decided to make my own prototype model and was inspired by an acronym disambiguation white paper from Courant Institute, NYU (Turtel and Shasha 2007). 2.1 Description Goal: Use an SVM and statistical analysis (of words/context) to properly classify ambiguous acronym definitions based on provided data. Acronym disambiguation is the process of determining the correct expansion/definition of an acronym in a given context These specific acronyms have multiple definitions, making them ambiguous when undefined Ambiguous sentences contain undefined acronyms, whereas unambiguous sentences contain both the acronyms and their expansions Example: In the given NASA abstracts, the acronym IMF is used with two separate definitions: Interplanetary Magnetic Field (32 instances) Intrinsic Mode Functions (14 instances) Given an ambiguous sentence containing an ambiguous acronym: Expressed in the IMFs, they have well-behaved Hilbert Transforms from which instantaneous frequencies can be calculated. The algorithm guesses what IMF stands for (in this case, Intrinsic Mode Functions) based on three different contexts: ambiguous, unambiguous, and total (the whole abstract). 2.1.1 Data Each line in processed_acronyms.jsonl is an acronym found to have more than one definition; there are 484. However, according to the dataset&#x2019;s description, this isn&#x2019;t quality data &#x2013; insofar as what is defined as a &#x201C;proper&#x201D; alternate definition/expansion (see Future Work section), along with a couple of wonky results too (see below). For example, &#x201C;TOMS&#x201D; isn&#x2019;t really an ambiguous acronym: Total Ozone Mapping Spectrometer Ozone Mapping Spectrometer ( This algorithm works for any legitimate ambiguous acronym within the dataset. Example usage: #!/usr/bin/env bash python3 nasa_svm.py &lt;acronym&gt; [--v] I could&#x2019;ve tailored the algorithm to sift through &#x2013; then run on &#x2013; every legitimate acronym, but for brevity&#x2019;s sake I selected 10 acronyms with 2 definitions, 5 acronyms with 3 definitions, and 1 acronym with 4 definitions (roughly paralleling the distribution): Acronyms 1. &quot;US&quot;: &apos;United States&apos;, &apos;Upper Stage&apos; 2. &quot;SST&quot;: &apos;Sea Surface Temperature&apos;, &apos;Shear Stress Transport&apos; 3. &quot;IMF&quot;: &apos;Interplanetary Magnetic Field&apos;, &apos;Intrinsic Mode Functions&apos; 4. &quot;VMS&quot;: &apos;Vertical Motion Simulator&apos;, &apos;Visual Motion Simulator&apos; 5. &quot;RMS&quot;: &apos;Remote Manipulator System&apos;, &apos;Root Mean Square&apos; 6. &quot;DOE&quot;: &apos;Department of Energy&apos;, &apos;Design of Experiments&apos; 7. &quot;NAS&quot;: &apos;National Airspace System&apos;, &apos;Numerical Aerodynamic Simulation&apos; 8. &quot;LET&quot;: &apos;Linear Energy Transfer&apos;, &apos;Link Evaluation Terminal&apos; 9. &quot;MLS&quot;: &apos;Microwave Limb Sounder&apos;, &apos;Microwave Landing System&apos; 10. &quot;RCS&quot;: &apos;Reaction Control System&apos;, &apos;Radar Cross Section&apos; 11. &quot;ISO&quot;: &apos;Infrared Space Observatory&apos;, &apos;International Standards Organization&apos;, &apos;Imaging Spectrometric Observatory&apos; 12. &quot;CM&quot;: &apos;Crew Module&apos;, &apos;Command Module&apos;, &apos;Configuration Management&apos; 13. &quot;PEM&quot;: &apos;Pressurized Excursion Module&apos;, &apos;Proton Exchange Membrane&apos;, &apos;Pacific Exploratory Mission&apos; 14. &quot;CRM&quot;: &apos;Common Research Model&apos;, &apos;Cockpit Resource Management&apos;, &apos;Crew Resource Management&apos; 15. &quot;LCC&quot;: &apos;Launch Control Center&apos;, &apos;Launch Commit Criteria&apos;, &apos;Life Cycle Cost&apos; 16. &quot;ATM&quot;: &apos;Air Traffic Management&apos;, &apos;Asynchronous Transfer Mode&apos;, &apos;Apollo Telescope Mount&apos;, &apos;Airborne Topographic Mapper&apos; 2.2 Algorithm Skeleton: INPUT: Acronym from NASA abstract Filter all relevant abstracts FOR each acronym definition: Find and extract all sentences containing acronym Separate into ambiguous and unambiguous Remove both the definitions and acronyms END Randomly sample test sentences containing ambiguous acronym Extract and remove these from training set Build feature vector of surrounding meaningful words for context Concat sentences for each definition as the n documents for tf-idf Train multi-class linear SVCs to yield word frequency coefficients Three models: ambiguous, unambiguous, and total contexts FOR each test sentence: Create feature vector and feed into model for prediction Grade predictions END OUTPUT: results in csv format for analysis CSV Output: The acronym along with the accuracies for the ambiguous, unambiguous, and total (combined) contexts. python3 nasa_svm.py DOE ## DOE 0.9491525423728814 0.9661016949152542 0.9661016949152542 Accuracies are used as the scoring metric as it&#x2019;s equivalent to the micro-weighted aggregate F-beta score given the class imbalance in the data (see Analysis). Verbose Output: Provides more insight such as the number of training examples for each definition, the confusion matrices, and F1-scores for each model (to show performance in each class). time python3 nasa_svm.py ISO --v ## ------------------------- ## Test set: (n = 30) ## Infrared Space Observatory: 23 ## International Standards Organization: 2 ## Imaging Spectrometric Observatory: 5 ## ## Ambiguous n: 120 slices: [93, 15, 13] ## Unambiguous n: 105 slices: [80, 14, 12] ## Combined n: 712 slices: [544, 99, 70] ## ## Guess (ambiguous) MCM &amp; F1: ## [[[ 5 2] ## [ 3 20]] ## ## [[28 0] ## [ 1 1]] ## ## [[21 4] ## [ 2 3]]] ## Infrared Space Observatory: 0.888888888888889 ## International Standards Organization: 0.6666666666666666 ## Imaging Spectrometric Observatory: 0.5 ## ## Guess (unambiguous) MCM &amp; F1: ## [[[ 6 1] ## [ 4 19]] ## ## [[27 1] ## [ 0 2]] ## ## [[21 4] ## [ 2 3]]] ## Infrared Space Observatory: 0.8837209302325583 ## International Standards Organization: 0.8 ## Imaging Spectrometric Observatory: 0.5 ## ## Guess (combined) MCM &amp; F1: ## [[[ 2 5] ## [ 1 22]] ## ## [[28 0] ## [ 1 1]] ## ## [[24 1] ## [ 4 1]]] ## Infrared Space Observatory: 0.8800000000000001 ## International Standards Organization: 0.6666666666666666 ## Imaging Spectrometric Observatory: 0.28571428571428575 ## ## Accuracy (ambiguous): 0.8 ## Accuracy (unambiguous): 0.8 ## Accuracy (combined): 0.8 ## ## real 0m4.267s ## user 0m3.655s ## sys 0m1.080s A bag of words model is implemented for the test samples, one for each of the context vocabularies. This allows for proper feature vectors to be created for classification. The SVM is implemented with scikit-learn as a support vector classifier (SVC) using a linear kernel. Model parameters (e.g., C) were chosen from the aforementioned white paper, therefore fine-tuning via cross-validation, etc., is not included here but proposed in the TODO section for when future improvements are implemented. The SVC uses a one-vs-one (OVO) shape for the decision function (see under the hood), which in the case of binary classification always considers the distance from the hyperplane. I.e., how &#x201C;deep&#x201D; the data point is in a specific class&#x2019; area This also contributes to why the algorithm works so well for 2 definitions but poorly for 3-4 (see Conclusions). 2.3 Analysis Given the slices shown in verbose output, almost every acronym seems to have a dominant definition which appears 2-5 times more often than the others. Since the test sentences are randomly sampled, I made sure that each definition is included for every test set. This class imbalance, along with the sample sizes for the training sets being pretty small, causes trouble when the model tries classifying acronyms with more than 2 definitions. I also added a classifier trained with all the sentences in the abstracts for each acronym as a combined context to see if stripping &#x201C;noise&#x201D; helped. My guess is that since the training sets are small, every bit of text matters. Collecting only the surrounding words in sentences containing acronyms (as opposed to using the whole abstract for example) should work best with a larger training set 2.3.1 Scripts For example, running the script 5 times and writing to csv files: #!/bin/zsh for acronym in US SST IMF VMS RMS DOE NAS LET MLS RCS ISO CM PEM CRM LCC ATM do printf &apos;Writing to ./nasa-svm_data/results/output_%s.txt\\n&apos; &quot;$acronym&quot; for i in {1..5} do python3 nasa_svm.py $acronym &gt;&gt; nasa-svm_data/results/output_${acronym}.csv done done Quick analysis using python: import csv import pandas as pd from glob import glob file_list = glob(&quot;nasa-svm_data/results/*.csv&quot;) for file in file_list: with open(file, newline=&apos;&apos;) as csvfile: buf = pd.DataFrame(csv.reader(csvfile, delimiter=&apos; &apos;, quotechar=&apos;|&apos;), columns=[&apos;acronym&apos;, &apos;acc_amb&apos;, &apos;acc_unamb&apos;,&apos;acc_comb&apos;]) if file == file_list[0]: grades = buf else: grades = pd.concat([grades, buf], axis=0) grades = grades.astype({&apos;acc_amb&apos;:float, &apos;acc_unamb&apos;:float, &apos;acc_comb&apos;:float}) # Take grouped averages and sort by ambiguous avgs = grades.groupby([&apos;acronym&apos;])[[&apos;acc_amb&apos;, &apos;acc_unamb&apos;, &apos;acc_comb&apos;]].mean() avgs = avgs.sort_values([&apos;acc_amb&apos;], ascending=False) # Average accuracies (33 randomly sampled test groups) print(avgs) ## acc_amb acc_unamb acc_comb ## acronym ## IMF 0.990676 0.904429 0.990676 ## MLS 0.982290 0.974813 0.979142 ## RCS 0.976874 0.974482 0.990431 ## US 0.964912 0.974482 0.998405 ## LET 0.964349 0.941176 0.998217 ## DOE 0.963020 0.954802 0.955316 ## SST 0.939394 0.909091 0.944056 ## NAS 0.926815 0.913665 0.923957 ## RMS 0.921212 0.943434 0.929293 ## LCC 0.746212 0.645833 0.700758 ## ISO 0.744444 0.762626 0.795960 ## VMS 0.741414 0.888889 0.751515 ## PEM 0.609626 0.597148 0.661319 ## CM 0.608586 0.624579 0.693603 ## CRM 0.582888 0.545455 0.549020 ## ATM 0.449811 0.462121 0.482008 # Average accuracies for 2 definitions print(avgs.head(10)[[&apos;acc_amb&apos;, &apos;acc_unamb&apos;, &apos;acc_comb&apos;]].mean()) ## acc_amb 0.937576 ## acc_unamb 0.913621 ## acc_comb 0.941025 ## dtype: float64 # Average accuracies for 3 definitions (and one 4) print(avgs.tail(5)[[&apos;acc_amb&apos;, &apos;acc_unamb&apos;, &apos;acc_comb&apos;]].mean()) ## acc_amb 0.598465 ## acc_unamb 0.623638 ## acc_comb 0.627493 ## dtype: float64 2.4 Conclusions Training the model on entire abstracts provided a marginal increase in accuracy over immediate contexts despite greatly increasing the vocabulary, which hints that most of it is noise. This also makes sense intuitively, given that NASA white paper abstracts are similar in structure. For 2 definitions this algorithm performs quite well given sparse vocabularies formed from small data. For more than 2 definitions we run into some problems, the main ones being small training sets and imbalanced class distributions. There aren&#x2019;t enough instances to properly train the models, but this is out of our hands. Simply put, the models need more examples of immediate context to capture the greater complexity presented in acronyms with 3-4 definitions &#x2013; which in turn would provide better hyperplanes to separate the definitions. Also given an OVO decision function, we view multi-label classification as several related binary classification tasks: It&#x2019;s harder to distinguish the remaining 2-3 &#x201C;lesser-known&#x201D; definitions from the &#x201C;main/common&#x201D; definition which has the highest frequency; the common definition &#x201C;encroaches,&#x201D; which tends towards more false negatives for the common and more false negatives/positives between the lesser-knowns. 2.4.1 TODO Results could be improved by implementing an unbiased classifier, perhaps with a one-vs-all/one-vs-rest (OVA/OVR) decision function (as opposed to the OVO used earlier). By default, the model assigns equal class weights because it assumes the data is evenly distributed between classes. For example, with an acronym that has 3 definitions: There is an approximate class label ratio of &#xA0;&#x223C;3:1:1\\ \\sim 3:1:1&#xA0;&#x223C;3:1:1, but some are different (e.g., &#xA0;&#x223C;1:1:3\\ \\sim 1:1:3&#xA0;&#x223C;1:1:3) Using the class_weight=&apos;balanced&apos; hyperparameter, the SVC decreases the weight of records in the &#x201C;common class&#x201D; in order to balance the weight of the whole class (e.g., 2:1:1 gives [0.75, 1.5, 1.5]) Would have to compare with manually calculated class weights based on each distribution (e.g., 2:1:1 gives [1, 2, 2]) Perform comparative metrics for OVO vs OVR given acronyms with 3-4 definitions 2.5 Future Work? There are a few hundred other acronyms available to play with. The following examples of acronyms could serve as inspiration for another (more complex) project than this one &#x2013; perhaps for: Learning grammar for either correction or more nuanced guesses (see CFD, TRMM, MSS, TES) Developing some similarity-based merging of classifications (see AVIRIS, NASA, JPL, CCD) Maybe even a combo of the aforementioned (see GEO) List of Acronyms - Nuanced Definitions: Including but not limited to: 1. &quot;CFD&quot;: &apos;Computational Fluid Dynamics&apos;, &apos;Computational fluid dynamics&apos; 2. &quot;TRMM&quot;: &apos;Tropical Rainfall Measuring Mission&apos;, &apos;Tropical Rainfall Measurement Mission&apos;, &apos;Tropical Rain Measuring Mission&apos; 3. &quot;MSS&quot;: &apos;Mobile Satellite Service&apos;, &apos;Mobile Servicing System&apos; 4. &quot;AVIRIS&quot;: &apos;Airborne Visible/Infrared Imaging Spectrometer&apos;, &apos;Airborne Visible and Infrared Imaging Spectrometer&apos; 5. &quot;TES&quot;: &apos;Thermal Emission Spectrometer&apos;, &apos;Tropospheric Emission Spectrometer&apos; 6. &quot;NASA&quot;: &apos;National Aeronautics &amp; Space Administration&apos;, &apos;National Aeronautic and Space Administration&apos; 7. &quot;JPL&quot;: &apos;Jet Propulsion Laboratory&apos;, &apos;Jet Propulsion Lab&apos; 8. &quot;CCD&quot;: &apos;Charge Coupled Device&apos;, &apos;Charge Coupled Devices&apos; 9. &quot;GEO&quot;: &apos;Geosynchronous Earth Orbit&apos;, &apos;geosynchronous Earth orbit&apos;, &apos;Group on Earth Observations&apos;, &apos;Geostationary Earth Orbit&apos;, &apos;geostationary Earth orbit&apos; Useful Metadata: Each acronym definition also has &#x201C;NASA terms&#x201D; attached to it, for example IMF comes with some &#x201C;additional context&#x201D; which can be utilized. 1. Interplanetary Magnetic Field: &apos;SOLAR MAGNETIC FIELD&apos;, &apos;SOLAR WIND&apos;, &apos;INTERPLANETARY MAGNETIC FIELDS&apos;, &apos;MAGNETIC FLUX&apos;, &apos;MAGNETIC PROBES&apos;, &apos;SPACE PROBES&apos; 2. Intrinsic Mode Functions: &apos;HILBERT TRANSFORMATION&apos;, &apos;SPECTRAL EMISSION&apos;, &apos;DECOMPOSITION&apos;, &apos;SPECTRUM ANALYSIS&apos;, &apos;TIME FUNCTIONS&apos;, &apos;FREQUENCY DISTRIBUTION&apos;, &apos;NONLINEARITY&apos;, &apos;TIME SERIES ANALYSIS&apos;, &apos;NONLINEAR SYSTEMS&apos;, &apos;NONLINEAR EQUATIONS&apos; Source Code The source code for this project can be downloaded or viewed and copied to the clipboard below: Download nasa_svm.py import json import re import nltk import pandas as pd import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, f1_score from sklearn.svm import SVC from functools import reduce import random import operator import sys #nltk.download(&apos;punkt&apos;) #nltk.download(&apos;stopwords&apos;) def preprocess(sentences, slices, names = None, tfidf = True, context = None): if context is None: context = [] for i, s in enumerate(slices): if i == 0: context.append(&apos; &apos;.join(sentences[:s])) else: context.append(&apos; &apos;.join(sentences[slices[i-1]:(s + slices[i-1])])) context = cleaner(context) if tfidf: if names is None: vectors, vocab = tf_idf(context) else: vectors, vocab = tf_idf(context, names) return vectors, vocab else: return context def cleaner(context): context = [re.sub(r&apos;\\w*\\d\\w*&apos;, &apos;&apos;, w) for w in context] context = [re.sub(r&apos;[^A-Za-z0-9 ]+&apos;, &apos;&apos;, w) for w in context] context = [re.sub(r&apos;\\s+&apos;, &apos; &apos;, w) for w in context] return context def filtered(sentence, keywords): for k in keywords: sentence = sentence.replace(k, &apos;&apos;) return sentence def tf_idf(context, names = None): vectorizer = TfidfVectorizer(stop_words=&apos;english&apos;, token_pattern=r&apos;(?u)\\b[A-Za-z]+\\b&apos;) vector = vectorizer.fit_transform(context) vocab = vectorizer.vocabulary_ tokens = vectorizer.get_feature_names_out() df_tfidf = pd.DataFrame(data=vector.toarray(), index=names, columns=tokens) return df_tfidf, vocab def bow(context, names = None, vocab = None): vectorizer = CountVectorizer(stop_words=&apos;english&apos;, token_pattern=r&apos;(?u)\\b[A-Za-z]+\\b&apos;, vocabulary=vocab) vector = vectorizer.fit_transform(context) tokens = vectorizer.get_feature_names_out() df_bow = pd.DataFrame(data=vector.toarray(), index=names, columns=tokens) return df_bow tokenizer = nltk.data.load(&apos;tokenizers/punkt/english.pickle&apos;) acronym_list = [&apos;US&apos;,&apos;SST&apos;,&apos;IMF&apos;,&apos;VMS&apos;,&apos;RMS&apos;,&apos;DOE&apos;,&apos;NAS&apos;,&apos;LET&apos;, &apos;MLS&apos;,&apos;RCS&apos;,&apos;ISO&apos;,&apos;CM&apos;,&apos;PEM&apos;,&apos;CRM&apos;,&apos;LCC&apos;,&apos;ATM&apos;] with open(&apos;nasa-svm_data/processed/processed_acronyms.jsonl&apos;, &apos;r&apos;) as json_acronyms, open(&apos;nasa-svm_data/raw/results_merged.jsonl&apos;, &apos;r&apos;) as json_corpus: if len(sys.argv) &gt; 1: if sys.argv[1] in acronym_list: acronym = sys.argv[1] else: print(&quot;Example Usage: python3 nasa_svm.py &lt;acronym&gt; [--v]&quot;) print(f&apos;Acronyms: {acronym_list}&apos;) exit() else: acronym = random.choice(acronym_list) json_list = list(json_acronyms) json_list_corpus = list(json_corpus) ambiguous, sentences, combined, names, terms = [], [], [], [], [] slices, slices_amb, slices_cmb, slices_amb_ind = [], [], [], [] guess, guess_amb, guess_cmb, grades = [], [], [], [] for json_str in json_list: result = json.loads(json_str) if result[&apos;acronym&apos;] == acronym: keywords = [result[&apos;definition&apos;] + &quot; (&quot; + result[&apos;acronym&apos;] + &quot;)&quot;, result[&apos;acronym&apos;] + &quot; (&quot; + result[&apos;definition&apos;] + &quot;)&quot;, result[&apos;acronym&apos;], result[&apos;definition&apos;]] names.append(result[&apos;definition&apos;]) # For all the abstracts where acronym variant is found for index in result[&apos;corpus_positions&apos;]: json_abstract = json_list_corpus[index] # Take abstract, split each into sentences abstract = json.loads(json_abstract)[&apos;description&apos;] parsed = tokenizer.tokenize(str(abstract)) # Filter sentences and feed into split bucket for sentence in parsed: # All of the sentences with keywords removed combined.append(sentence) combined[-1] = filtered(combined[-1], keywords) if sentence.find(keywords[0]) != -1 or sentence.find(keywords[1]) != -1: sentences.append(sentence) # Remove acronyms and their definitions sentences[-1] = filtered(sentences[-1], keywords) # Change &apos;if&apos; below back to elif to have every sentence containing # both definition/acronym pair AND undefined acronyms # considered as only part of [sentences] (NOT also ambiguous) # also change sentences[-1] below back to sentence if sentence.find(keywords[2]) != -1: ambiguous.append(sentences[-1]) ambiguous[-1] = filtered(ambiguous[-1], keywords) # Ambiguous slices w.r.t. definitions slices_amb_ind.append(index) elif sentence.find(keywords[2]) != -1: ambiguous.append(sentence) ambiguous[-1] = filtered(ambiguous[-1], keywords) # Ambiguous slices w.r.t. definitions slices_amb_ind.append(index) # Slices based on which acronym sentence belongs to if len(slices_amb) == 0: slices_amb.append(len(slices_amb_ind)) slices.append(len(sentences)) slices_cmb.append(len(combined)) elif len(slices_amb) == 1: slices_amb.append(len(slices_amb_ind) - slices_amb[0] + 1) slices.append(len(sentences) - slices[0] + 1) slices_cmb.append(len(combined) - slices_cmb[0] + 1) else: # More than 2 defintiions slices_amb.append(len(slices_amb_ind) - reduce(operator.add, slices_amb) + 1) slices.append(len(sentences) - reduce(operator.add, slices) + 1) slices_cmb.append(len(combined) - reduce(operator.add, slices_cmb) + 1) # Optional additional contexutal terms for each definition #terms.append(&apos; &apos;.join(map(str, cleaner(json.loads(json_abstract)[&apos;subject.NASATerms&apos;])))) # Ensure at least 2 samples from each definition are extracted good_batch = False while not good_batch: testing_set = random.sample(ambiguous, round(len(ambiguous)/5)) testing_ind, key = [], [] # Determine which acronym the random sample belongs to for t in testing_set: if any(t in testing_set for t in ambiguous): testing_ind.append(ambiguous.index(t)) for count, i in enumerate(slices_amb): start = len(ambiguous) - sum(slices_amb[count:], -1) if ambiguous.index(t) in range(start, i+start): key.append(names[count]) key_counts = {i:key.count(i) for i in names} good_count = {k:v for (k,v) in key_counts.items() if v &gt; 1} if len(key_counts) == len(good_count): good_batch = True # Update slices key_counts = {i:key.count(i) for i in names} slices_amb = [a_i - b_i for a_i, b_i in zip(slices_amb, key_counts.values())] slices_cmb = [a_i - b_i for a_i, b_i in zip(slices_cmb, key_counts.values())] # Remove sample from training set for t in testing_set: if any(t in testing_set for t in ambiguous): ambiguous.remove(t) if any(t in testing_set for t in combined): combined.remove(t) # Build models context, vocab = preprocess(sentences, slices, names) model = SVC(C=1., kernel=&apos;linear&apos;, decision_function_shape=&apos;ovo&apos;) model.fit(context, names) context_amb, vocab_amb = preprocess(ambiguous, slices_amb, names) model_amb = SVC(C=1., kernel=&apos;linear&apos;, decision_function_shape=&apos;ovo&apos;) model_amb.fit(context_amb, names) context_cmb, vocab_cmb = preprocess(combined, slices_cmb, names) model_cmb = SVC(C=1., kernel=&apos;linear&apos;, decision_function_shape=&apos;ovo&apos;) model_cmb.fit(context_cmb, names) # Prepare tests for prediction testing_set = cleaner(testing_set) # Bag of words for all test sentences df = bow(testing_set, vocab=vocab) df_ambig = bow(testing_set, vocab=vocab_amb) df_comb = bow(testing_set, vocab=vocab_cmb) for i in range(len(testing_set)): guess.append(model.predict(df.loc[i].to_frame().T)) guess_amb.append(model_amb.predict(df_ambig.loc[i].to_frame().T)) guess_cmb.append(model_cmb.predict(df_comb.loc[i].to_frame().T)) results = pd.DataFrame([guess_amb, guess, guess_cmb,key], index=[&apos;Guess (ambiguous)&apos;, &apos;Guess (unambiguous)&apos;, &apos;Guess (combined)&apos;, &apos;Correct Answer&apos;]) for i in results.index[:3]: grades.append(np.where(results.loc[&apos;Correct Answer&apos;] == results.loc[i], True, False)) if len(sys.argv) &gt; 2: if sys.argv[2] == &apos;--v&apos;: print(&quot;-------------------------&quot;) print(f&apos;Test set: (n = {len(testing_set)})&apos;) for key, value in key_counts.items(): print(f&apos;{key}: {value}&apos;) print(f&apos;\\nAmbiguous n: {len(ambiguous)} \\t slices: {slices_amb}&apos;) print(f&apos;Unambiguous n: {len(sentences)} \\t slices: {slices}&apos;) print(f&apos;Combined n: {len(combined)} \\t slices: {slices_cmb}&apos;) if len(names) &gt; 2: for i in results.index[:3]: print(f&apos;\\n{i} MCM &amp; F1:&apos;) print(multilabel_confusion_matrix(list(results.loc[&apos;Correct Answer&apos;]), list(results.loc[i]), labels=names)) f = dict(zip(names, f1_score(list(results.loc[&apos;Correct Answer&apos;]), list(results.loc[i]), average=None, labels=names))) for key, value in f.items(): print(f&apos;{key}: {value}&apos;) else: for i in results.index[:3]: print(f&apos;\\n{i} CM &amp; F1:&apos;) print(confusion_matrix(list(results.loc[&apos;Correct Answer&apos;]), list(results.loc[i]))) f = dict(zip(names, f1_score(list(results.loc[&apos;Correct Answer&apos;]), list(results.loc[i]), average=None, labels=names))) for key, value in f.items(): print(f&apos;{key}: {value}&apos;) print(f&apos;\\nAccuracy (ambiguous): {sum(grades[0])/len(testing_set)}&apos;) print(f&apos;Accuracy (unambiguous): {sum(grades[1])/len(testing_set)}&apos;) print(f&apos;Accuracy (combined): {sum(grades[2])/len(testing_set)}&apos;) else: print(acronym, sum(grades[0])/len(testing_set), sum(grades[1])/len(testing_set), sum(grades[2])/len(testing_set)) References "],["google-capstone.html", "Chapter 3 Google Capstone [DA|R] 3.1 Description 3.2 Ask 3.3 Prepare / Process 3.4 Analyze 3.5 Share &amp; Act (Final Report)", " Chapter 3 Google Capstone [DA|R] In my free time, I audited the Google Data Analytics courses and completed its capstone. 3.1 Description Performed data analysis for a fictional bike-share company to illustrate the data analysis process learned from the Google Data Analytics Professional Certificate: Ask: Business objective, questions, challenges Prepare: Data generation, collection, storage, and management Process Data cleaning, ensuring data integrity Analyze: Data exploration, finding patterns/trends Share: Interpreting and communicating results Act: Using insights to solve the problem 3.1.1 Scenario Cyclistic&#x2019;s director of marketing believes the company&#x2019;s future success depends on maximizing the number of annual memberships. Customers who purchase single-ride or full-day passes are referred to as casual riders. Your team wants to understand how casual riders and annual members use Cyclistic bikes differently From these insights, a new marketing strategy will be designed to convert casuals into members. 3.2 Ask Three questions will guide the future marketing program. The director assigned me the first: How do annual members and casual riders use Cyclistic bikes differently? Why would casual riders buy Cyclistic annual memberships? How can Cyclistic use digital media to influence casual riders to become members? The task is to produce a report with the following deliverables: A clear statement of the business task A description of all data sources and software used Documentation of any cleaning or manipulation of data A summary of the analysis Supporting visualizations and key findings Recommendations based on analysis 3.2.1 Objective Analyze Cyclistic riders&#x2019; usage patterns a for novel marketing membership conversion program. 3.3 Prepare / Process The (fictional) historical data, divvy-tripdata, is publicly available and hosted on AWS. I used the Amazon Web Services Command Line Interface (AWS CLI) to scrape a year&#x2019;s worth of data (August 2021 - July 2022 inclusive): #!/bin/zsh # Bucket name is simply from url: &lt;bucket_name&gt;.s3.amazonaws.com # First exclude all objects, then include those needed # This will download the files into current directory, then unzip and rm zip aws s3 sync s3://divvy-tripdata . \\ --exclude &apos;*&apos; \\ --include &apos;20210[8-9]*.zip&apos; \\ --include &apos;20211[0-2]*.zip&apos; \\ --include &apos;20220[1-7]*.zip&apos; \\ &amp;&amp; unzip &apos;*.zip&apos; &amp;&amp; rm *.zip &amp;&amp; rm -rf __MACOSX/ I baked the following packages into some boilerplate code, then defined any necessary functions for pre-processing: packages &lt;- c(&quot;tidyverse&quot;, &quot;here&quot;, &quot;skimr&quot;, &quot;janitor&quot;, &quot;Hmisc&quot;, &quot;geosphere&quot;, &quot;sjmisc&quot;, &quot;lubridate&quot;, &quot;scales&quot;, &quot;RColorBrewer&quot;, &quot;ggmap&quot;, &quot;gridExtra&quot;) for (package in packages) { if (!require(package, character.only = TRUE)) { print(&quot;Installing package(s)&quot;) install.packages(package, repos = &quot;http://cran.us.r-project.org&quot;) library(package, character.only = TRUE) if (require(package, character.only = TRUE)) { print(&quot;Package(s) installed and loaded&quot;) } else { stop(&quot;Could not install package(s)&quot;) } } } # dplyr case_when + factors fct_case_when &lt;- function(...) { args &lt;- as.list(match.call()) levels &lt;- sapply(args[-1], function(f) f[[3]]) # extract RHS of formula levels &lt;- levels[!is.na(levels)] factor(case_when(...), levels=levels) } 3.3.1 Pre-Processing with R My workflow to ensure the data&#x2019;s integrity; cleaning and formatting it such that it&#x2019;s ready for analysis: # Store names of the files in list, path = . (default) files = list.files(pattern = &quot;*tripdata.csv&quot;) # Use those names to read in the csvs and properly rename them bike_data_raw &lt;- lapply(files, read_csv, show_col_types = FALSE) %&gt;% setNames(paste0(substring(files,1,6), &quot;_bike_data&quot;)) # Convert relevant data to factors and merge list into a single dataframe bike_data_raw &lt;- bike_data_raw %&gt;% lapply(mutate, member_casual = as.factor(str_to_title(member_casual)), rideable_type = as.factor(rideable_type)) %&gt;% bind_rows() glimpse(bike_data_raw) ## Rows: 5,901,463 ## Columns: 13 ## $ ride_id &lt;chr&gt; &quot;99103BB87CC6C1BB&quot;, &quot;EAFCCCFB0A3FC5A1&quot;, &quot;9EF4F46C57&#x2026; ## $ rideable_type &lt;fct&gt; electric_bike, electric_bike, electric_bike, electr&#x2026; ## $ started_at &lt;dttm&gt; 2021-08-10 17:15:49, 2021-08-10 17:23:14, 2021-08-&#x2026; ## $ ended_at &lt;dttm&gt; 2021-08-10 17:22:44, 2021-08-10 17:39:24, 2021-08-&#x2026; ## $ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,&#x2026; ## $ start_station_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,&#x2026; ## $ end_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, &quot;Clark St &amp; Grace St&quot;, &#x2026; ## $ end_station_id &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, &quot;TA1307000127&quot;, NA, NA,&#x2026; ## $ start_lat &lt;dbl&gt; 41.77000, 41.77000, 41.95000, 41.97000, 41.79000, 4&#x2026; ## $ start_lng &lt;dbl&gt; -87.68000, -87.68000, -87.65000, -87.67000, -87.600&#x2026; ## $ end_lat &lt;dbl&gt; 41.77000, 41.77000, 41.97000, 41.95000, 41.77000, 4&#x2026; ## $ end_lng &lt;dbl&gt; -87.68000, -87.63000, -87.66000, -87.65000, -87.620&#x2026; ## $ member_casual &lt;fct&gt; Member, Member, Member, Member, Member, Member, Mem&#x2026; # Columns containing NAs colnames(bike_data_raw)[colSums(is.na(bike_data_raw)) &gt; 0] ## [1] &quot;start_station_name&quot; &quot;start_station_id&quot; &quot;end_station_name&quot; ## [4] &quot;end_station_id&quot; &quot;end_lat&quot; &quot;end_lng&quot; # Drop NA bike_data_raw &lt;- drop_na(bike_data_raw) # Check if there are duplicate ride_IDs if(n_distinct(bike_data_raw$ride_id) != nrow(bike_data_raw)) { # Remove duplicate rows bike_data_raw &lt;- bike_data_raw %&gt;% distinct(ride_id, .keep_all = TRUE) } # Extract columns containing character vectors tmp_char_cols &lt;- bike_data_raw[, sapply(bike_data_raw, class) == &apos;character&apos;] colnames(tmp_char_cols) ## [1] &quot;ride_id&quot; &quot;start_station_name&quot; &quot;start_station_id&quot; ## [4] &quot;end_station_name&quot; &quot;end_station_id&quot; # Find which rows are test rows tmp_test_rows &lt;- which( rowSums( `dim&lt;-`(grepl(&quot;TEST|TESTING&quot;, as.matrix(tmp_char_cols)), dim(tmp_char_cols)) ) &gt; 0 ) # View and confirm (subsetted by id for example) unique(tmp_char_cols[tmp_test_rows, &apos;end_station_id&apos;]) ## # A tibble: 344 &#xD7; 1 ## end_station_id ## &lt;chr&gt; ## 1 Hubbard Bike-checking (LBS-WH-TEST) ## 2 13073 ## 3 18067 ## 4 637 ## 5 TA1307000159 ## 6 13132 ## 7 TA1309000032 ## 8 13434 ## 9 TA1305000041 ## 10 13256 ## # &#x2026; with 334 more rows # Store clean data and clean workspace bike_data &lt;- bike_data_raw[-tmp_test_rows, ] rm(files, bike_data_raw, list = ls(pattern=&apos;^tmp_&apos;)) glimpse(bike_data) ## Rows: 4,628,134 ## Columns: 13 ## $ ride_id &lt;chr&gt; &quot;DD06751C6019D865&quot;, &quot;79973DC3B232048F&quot;, &quot;0249AD4B25&#x2026; ## $ rideable_type &lt;fct&gt; classic_bike, classic_bike, classic_bike, classic_b&#x2026; ## $ started_at &lt;dttm&gt; 2021-08-08 17:21:26, 2021-08-27 08:53:52, 2021-08-&#x2026; ## $ ended_at &lt;dttm&gt; 2021-08-08 17:25:37, 2021-08-27 09:18:29, 2021-08-&#x2026; ## $ start_station_name &lt;chr&gt; &quot;Desplaines St &amp; Kinzie St&quot;, &quot;Larrabee St &amp; Armitag&#x2026; ## $ start_station_id &lt;chr&gt; &quot;TA1306000003&quot;, &quot;TA1309000006&quot;, &quot;13157&quot;, &quot;13042&quot;, &quot;&#x2026; ## $ end_station_name &lt;chr&gt; &quot;Kingsbury St &amp; Kinzie St&quot;, &quot;Michigan Ave &amp; Oak St&quot;&#x2026; ## $ end_station_id &lt;chr&gt; &quot;KA1503000043&quot;, &quot;13042&quot;, &quot;13157&quot;, &quot;13042&quot;, &quot;13042&quot;,&#x2026; ## $ start_lat &lt;dbl&gt; 41.88872, 41.91808, 41.87773, 41.90096, 41.90096, 4&#x2026; ## $ start_lng &lt;dbl&gt; -87.64445, -87.64375, -87.65479, -87.62378, -87.623&#x2026; ## $ end_lat &lt;dbl&gt; 41.88918, 41.90096, 41.87773, 41.90096, 41.90096, 4&#x2026; ## $ end_lng &lt;dbl&gt; -87.63851, -87.62378, -87.65479, -87.62378, -87.623&#x2026; ## $ member_casual &lt;fct&gt; Member, Member, Member, Casual, Casual, Casual, Cas&#x2026; 3.3.2 Feature Engineering I used the geosphere package (Hijmans 2021) to compute the distance between starting and ending coordinates and implemented lubridate (Spinu, Grolemund, and Wickham 2021) to parse starting dates of trips into buckets. The previously defined function fct_case_when() allows me to further classify the month bucket into factors. # Features for data integrity: trip_distance (miles), speed (mph) # The rest are engineered for deeper analysis bike_data &lt;- bike_data %&gt;% mutate(trip_duration = as.numeric(difftime(ended_at, started_at, units = &quot;hours&quot;)), trip_distance = distHaversine(cbind(start_lng, start_lat), cbind(end_lng, end_lat))/1609.35, speed = trip_distance/trip_duration, trip_hour = hour(started_at), trip_day = wday(started_at, label = TRUE), trip_month = month(started_at, label = TRUE), afternoon = trip_hour %in% seq(12,18), weekend = as.integer(trip_day) %in% c(1, 6, 7), season = fct_case_when(as.integer(trip_month) %in% c(12, 1, 2) ~ &quot;Winter&quot;, as.integer(trip_month) %in% seq(3, 5) ~ &quot;Spring&quot;, as.integer(trip_month) %in% seq(6, 8) ~ &quot;Summer&quot;, as.integer(trip_month) %in% seq(9, 11) ~ &quot;Fall&quot;), route = paste(start_station_name, end_station_name, sep = &quot; to &quot;) ) # Data verification: Are there any immediately &quot;impossible&quot; outliers? # - nonpositive trip durations and extreme speeds # Caveats: speed = dist/time used, idea is to appromixate the beeline # speed one must go to reach destination in given time # and compare to max speed possible on bike (~30 mph), # instead of an accurate formula &amp; general model which # takes into account location, stops, turns, etc. glimpse(bike_data %&gt;% select(trip_distance, trip_duration, speed) %&gt;% filter(speed &gt;= 30 | trip_duration &lt;= 0)) ## Rows: 721 ## Columns: 3 ## $ trip_distance &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.025981785, 0.00&#x2026; ## $ trip_duration &lt;dbl&gt; -0.0100000000, 0.0000000000, 0.0000000000, 0.0005555556,&#x2026; ## $ speed &lt;dbl&gt; 0.0000000, NaN, NaN, 46.7672134, -0.0793921, NaN, NaN, 0&#x2026; bike_data &lt;- bike_data %&gt;% filter(speed &lt; 30) %&gt;% filter(trip_duration &gt; 0) glimpse(bike_data) ## Rows: 4,627,413 ## Columns: 23 ## $ ride_id &lt;chr&gt; &quot;DD06751C6019D865&quot;, &quot;79973DC3B232048F&quot;, &quot;0249AD4B25&#x2026; ## $ rideable_type &lt;fct&gt; classic_bike, classic_bike, classic_bike, classic_b&#x2026; ## $ started_at &lt;dttm&gt; 2021-08-08 17:21:26, 2021-08-27 08:53:52, 2021-08-&#x2026; ## $ ended_at &lt;dttm&gt; 2021-08-08 17:25:37, 2021-08-27 09:18:29, 2021-08-&#x2026; ## $ start_station_name &lt;chr&gt; &quot;Desplaines St &amp; Kinzie St&quot;, &quot;Larrabee St &amp; Armitag&#x2026; ## $ start_station_id &lt;chr&gt; &quot;TA1306000003&quot;, &quot;TA1309000006&quot;, &quot;13157&quot;, &quot;13042&quot;, &quot;&#x2026; ## $ end_station_name &lt;chr&gt; &quot;Kingsbury St &amp; Kinzie St&quot;, &quot;Michigan Ave &amp; Oak St&quot;&#x2026; ## $ end_station_id &lt;chr&gt; &quot;KA1503000043&quot;, &quot;13042&quot;, &quot;13157&quot;, &quot;13042&quot;, &quot;13042&quot;,&#x2026; ## $ start_lat &lt;dbl&gt; 41.88872, 41.91808, 41.87773, 41.90096, 41.90096, 4&#x2026; ## $ start_lng &lt;dbl&gt; -87.64445, -87.64375, -87.65479, -87.62378, -87.623&#x2026; ## $ end_lat &lt;dbl&gt; 41.88918, 41.90096, 41.87773, 41.90096, 41.90096, 4&#x2026; ## $ end_lng &lt;dbl&gt; -87.63851, -87.62378, -87.65479, -87.62378, -87.623&#x2026; ## $ member_casual &lt;fct&gt; Member, Member, Member, Casual, Casual, Casual, Cas&#x2026; ## $ trip_duration &lt;dbl&gt; 0.069722222, 0.410277778, 0.010277778, 0.078333333,&#x2026; ## $ trip_distance &lt;dbl&gt; 0.307632959, 1.568416347, 0.000000000, 0.000000000,&#x2026; ## $ speed &lt;dbl&gt; 4.412265551, 3.822815741, 0.000000000, 0.000000000,&#x2026; ## $ trip_hour &lt;int&gt; 17, 8, 12, 16, 15, 10, 23, 22, 18, 13, 21, 11, 18, &#x2026; ## $ trip_day &lt;ord&gt; Sun, Fri, Sun, Thu, Mon, Mon, Sat, Fri, Mon, Thu, W&#x2026; ## $ trip_month &lt;ord&gt; Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, Aug, A&#x2026; ## $ afternoon &lt;lgl&gt; TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE,&#x2026; ## $ weekend &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, &#x2026; ## $ season &lt;fct&gt; Summer, Summer, Summer, Summer, Summer, Summer, Sum&#x2026; ## $ route &lt;chr&gt; &quot;Desplaines St &amp; Kinzie St to Kingsbury St &amp; Kinzie&#x2026; 3.4 Analyze The following is a &#x201C;slideshow&#x201D; of relevant statistics and visuals along with their code. To keep it clean, key observations are listed afterward (with cross-references) in the final report. In normal reports or slideshows, the graphic wouldn&#x2019;t have the code attached to it &#x2013; just the caption and key observation(s). 3.4.1 Basic Descriptive Statistics bike_data %&gt;% select(start_lat:end_lng, member_casual:trip_hour) %&gt;% group_by(member_casual) %&gt;% descr(show = c(&quot;mean&quot;, &quot;sd&quot;, &quot;trimmed&quot;, &quot;range&quot;, &quot;iqr&quot;, &quot;skew&quot;)) ## ## ## Basic descriptive statistics ## ## ## Grouped by: Casual ## ## var mean sd trimmed range iqr skew ## start_lat 41.90 0.04 41.90 0.42 (41.65-42.06) 0.04451399 -0.36 ## start_lng -87.64 0.03 -87.64 0.3 (-87.83--87.53) 0.02905797 -0.85 ## end_lat 41.90 0.04 41.90 0.52 (41.65-42.17) 0.04566567 -0.37 ## end_lng -87.64 0.03 -87.64 0.3 (-87.83--87.53) 0.03058697 -0.84 ## trip_duration 0.44 1.77 0.31 693.82 (0-693.82) 0.31944444 161.70 ## trip_distance 1.36 1.23 1.19 19.85 (0-19.85) 1.20339742 2.19 ## speed 5.03 3.14 4.97 29.44 (0-29.44) 4.49007265 0.09 ## trip_hour 14.57 5.05 14.95 23 (0-23) 6.00000000 -0.77 ## ## ## Grouped by: Member ## ## var mean sd trimmed range iqr skew ## start_lat 41.90 0.04 41.90 0.42 (41.65-42.06) 0.04698683 -0.15 ## start_lng -87.65 0.02 -87.64 0.3 (-87.83--87.53) 0.02757026 -0.51 ## end_lat 41.90 0.04 41.90 0.42 (41.65-42.06) 0.04708300 -0.15 ## end_lng -87.65 0.02 -87.64 0.3 (-87.83--87.53) 0.02757026 -0.52 ## trip_duration 0.21 0.29 0.18 24.88 (0-24.88) 0.17111111 31.47 ## trip_distance 1.27 1.11 1.09 17.1 (0-17.1) 1.11475723 2.05 ## speed 6.71 2.76 6.81 29.98 (0-29.98) 3.09570863 -0.23 ## trip_hour 13.94 4.92 14.13 23 (0-23) 8.00000000 -0.43 3.4.2 Viz: Trip Behavior I used the RColorBrewer package (Neuwirth 2022) for palettes. # Pie chart: member vs. casual bike_data %&gt;% select(member_casual) %&gt;% group_by(member_casual) %&gt;% count() %&gt;% ungroup %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = member_casual)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) + geom_text(aes(label = perc), size = 5, position = position_fill(vjust = 0.5)) + theme_void() + labs(title = &quot;Casual Riders vs. Annual Members&quot;) + theme(legend.title = element_blank(), legend.position = &quot;bottom&quot;, legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) Figure 3.1: Membership Status # Bar chart: member vs. casual, days of week bike_data %&gt;% select(member_casual, trip_day) %&gt;% group_by(member_casual, trip_day) %&gt;% count(trip_day) %&gt;% ggplot(aes(x = trip_day, y = n, fill = member_casual)) + geom_col() + facet_wrap( ~member_casual, ncol = 1) + scale_y_continuous(labels = comma) + labs(title = &quot;Trip Behavior: Days of the Week&quot;, x = &quot;\\nTrip Day&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), plot.title = element_text(color = &quot;black&quot;, size = 22)) Figure 3.2: Casual vs.&#xA0;Member - Rides per Day # Pie chart: weekend/weekday use bike_data %&gt;% select(member_casual, weekend) %&gt;% group_by(member_casual) %&gt;% count(weekend) %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = weekend)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) + facet_wrap( ~ member_casual) + geom_text(aes(label = perc), position = position_fill(vjust = 0.5)) + theme_void() + scale_fill_manual(values = c(&quot;#FBB4AE&quot;,&quot;#7FC97F&quot;)) + labs(title = &quot;Weekend Use\\n&quot;) + theme(legend.title = element_blank(), legend.position = &quot;bottom&quot;, legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), strip.text.x = element_text(size = 14), plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) Figure 3.3: Casual vs.&#xA0;Member - Weekend # Bar chart: member vs. casual, rides by month bike_data %&gt;% select(member_casual, trip_month) %&gt;% group_by(member_casual, trip_month) %&gt;% count(trip_month) %&gt;% ggplot(aes(x = trip_month, y = n, fill = member_casual)) + geom_col() + scale_y_continuous(labels = comma) + labs(title = &quot;Trip Behavior: Months of the Year&quot;, x = &quot;\\nTrip Month&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), plot.title = element_text(color = &quot;black&quot;, size = 22)) Figure 3.4: Casual vs.&#xA0;Member - Rides per Month # Pie Chart: member vs. casual, summer use bike_data %&gt;% select(member_casual, season) %&gt;% group_by(member_casual) %&gt;% count(summer = season == &quot;Summer&quot;) %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = summer)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) + facet_wrap( ~ member_casual) + geom_text(aes(label = perc), position = position_fill(vjust = 0.5)) + theme_void() + scale_fill_manual(values = c(&quot;#FBB4AE&quot;,&quot;#7FC97F&quot;)) + labs(title = &quot;Summer Use\\n&quot;) + theme(legend.title = element_blank(), legend.position = &quot;bottom&quot;, legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), strip.text.x = element_text(size = 14), plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) Figure 3.5: Casual vs.&#xA0;Member - Summer # Bar chart: member vs. casual, rides by hour (total) bike_data %&gt;% select(member_casual, trip_hour) %&gt;% group_by(member_casual) %&gt;% count(trip_hour) %&gt;% ggplot(aes(x = trip_hour, y = n, fill = member_casual)) + geom_col() + scale_y_continuous(labels = comma) + scale_x_continuous(breaks = seq(0, 23)) + labs(title = &quot;Trip Behavior: Rides by Hour&quot;, x = &quot;\\nTrip Hour&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.5, &quot;cm&quot;), legend.text = element_text(size = 10), plot.title = element_text(color = &quot;black&quot;, size = 22)) Figure 3.6: Casual vs.&#xA0;Member - Rides per Hour # Pie chart: member vs casual, afternoon use (12-6 pm) bike_data %&gt;% select(member_casual, afternoon) %&gt;% group_by(member_casual) %&gt;% count(afternoon) %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = afternoon)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) + facet_wrap( ~ member_casual) + geom_text(aes(label = perc), position = position_fill(vjust = 0.5)) + theme_void() + scale_fill_manual(values = c(&quot;#FBB4AE&quot;,&quot;#7FC97F&quot;)) + labs(title = &quot;Afternoon Use\\n&quot;) + theme(legend.title = element_blank(), legend.position = &quot;bottom&quot;, legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), strip.text.x = element_text(size = 14), plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) Figure 3.7: Casual vs.&#xA0;Member - Afternoon # Bar chart: member vs casual, rides by hour (by day) bike_data %&gt;% select(member_casual, trip_day, trip_hour) %&gt;% group_by(member_casual, trip_day) %&gt;% count(trip_hour) %&gt;% ggplot(aes(x = trip_hour, y = n, fill = member_casual)) + geom_col() + scale_y_continuous(labels = comma) + facet_wrap(~ trip_day) + labs(title = &quot;Trip Behavior: Rides by Hour&quot;, subtitle = &quot;Days of the Week&quot;, x = &quot;Trip Hour&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), plot.subtitle = element_text(size = 12), legend.key.size = unit(0.5, &quot;cm&quot;), legend.text = element_text(size = 10), plot.title = element_text(color = &quot;black&quot;, size = 22)) Figure 3.8: Casual vs.&#xA0;Member - Rides per Hour Each Day # Bar chart: member vs casual, rides by hour (by month) bike_data %&gt;% select(member_casual, trip_month, trip_hour) %&gt;% group_by(member_casual, trip_month) %&gt;% count(trip_hour) %&gt;% ggplot(aes(x = trip_hour, y = n, fill = member_casual)) + geom_col() + scale_y_continuous(labels = comma) + facet_wrap(~ trip_month) + labs(title = &quot;Trip Behavior: Rides by Hour&quot;, subtitle = &quot;Months of the Year&quot;, x = &quot;\\nTrip Hour&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), plot.subtitle = element_text(size = 12), legend.key.size = unit(0.5, &quot;cm&quot;), legend.text = element_text(size = 10), plot.title = element_text(color = &quot;black&quot;, size = 22)) Figure 3.9: Casual vs.&#xA0;Member - Rides per Hour Each Month # Pie chart by season bike_data %&gt;% select(member_casual, season) %&gt;% group_by(member_casual) %&gt;% count(season) %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = season)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;) + facet_wrap( ~ member_casual) + geom_text(aes(x = 1.675, label = perc), position = position_fill(vjust=0.5), size = 3) + theme_void() + scale_fill_manual(values = c(&quot;#C7E9B4&quot;, &quot;#7FCDBB&quot;, &quot;#41B6C4&quot;, &quot;#1D91C0&quot;)) + labs(title = &quot;Seasonal Use\\n&quot;) + theme(legend.title = element_blank(), legend.position = &quot;bottom&quot;, legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), strip.text.x = element_text(size = 14), plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) Figure 3.10: Casual vs.&#xA0;Member - Seasons 3.4.3 Viz: Bike Choice # Pie chart: types of bikes bike_data %&gt;% select(rideable_type) %&gt;% count(rideable_type) %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = rideable_type)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;, direction = -1) + geom_text(aes(x = 1.1, label = perc), position = position_fill(vjust = 0.5), size = 3.5) + theme_void() + scale_fill_manual(name = &quot;&quot;, values = c(&quot;#E5C494&quot;, &quot;#B3B3B3&quot;, &quot;#FFD92F&quot;), labels = c(&quot;Classic&quot;, &quot;Docked&quot;, &quot;Electric&quot;)) + labs(title = &quot;Bike Choice (Overall)&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), legend.position = &quot;bottom&quot;, plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) + guides(fill = guide_legend(reverse = TRUE, title.position = &quot;top&quot;)) Figure 3.11: Total Bike Choice # Bar chart: member vs. casual, bike choice bike_data %&gt;% select(rideable_type, member_casual) %&gt;% group_by(rideable_type, member_casual) %&gt;% count(rideable_type) %&gt;% ggplot(aes(x = member_casual, y = n, fill = rideable_type)) + geom_col() + geom_text(aes(label = comma(n)), position = position_stack(vjust =0.5), size = 3) + scale_y_continuous(labels = comma) + scale_fill_manual(values = c(&quot;#E5C494&quot;, &quot;#B3B3B3&quot;, &quot;#FFD92F&quot;), name = &quot;Bike Type&quot;, labels = c(&quot;Classic&quot;, &quot;Docked&quot;, &quot;Electric&quot;)) + labs(title = &quot;Bike Choice: Casuals vs. Members&quot;, x = &quot;&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), axis.text.x = element_text(size = 14), plot.title = element_text(color = &quot;black&quot;, size = 22)) Figure 3.12: Casual vs.&#xA0;Member - Choice # Pie chart: types of bikes bike_data %&gt;% select(rideable_type, member_casual) %&gt;% group_by(member_casual) %&gt;% count(rideable_type) %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = rideable_type)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;, direction = -1) + facet_wrap( ~ member_casual) + geom_text(aes(x = 1.05, label = perc), position = position_fill(vjust=0.5)) + theme_void() + scale_fill_manual(name = &quot;&quot;, values = c(&quot;#E5C494&quot;, &quot;#B3B3B3&quot;, &quot;#FFD92F&quot;), labels = c(&quot;Classic&quot;, &quot;Docked&quot;, &quot;Electric&quot;)) + labs(title = &quot;Bike Choice\\n&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.65, &quot;cm&quot;), legend.text = element_text(size = 12), legend.position = &quot;bottom&quot;, strip.text.x = element_text(size = 14), plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) + guides(fill = guide_legend(reverse = TRUE, title.position = &quot;top&quot;)) Figure 3.13: (Pie) Casual vs.&#xA0;Member - Choice # Bar Chart: rideable_type / day bike_data %&gt;% select(trip_day, rideable_type, member_casual) %&gt;% group_by(trip_day, rideable_type, member_casual) %&gt;% count(rideable_type) %&gt;% ggplot(aes(x = member_casual, y = n, fill = rideable_type)) + geom_col() + facet_wrap(~ trip_day) + scale_y_continuous(labels = comma) + scale_fill_manual(values = c(&quot;#E5C494&quot;, &quot;#B3B3B3&quot;, &quot;#FFD92F&quot;), labels = c(&quot;Classic&quot;, &quot;Docked&quot;, &quot;Electric&quot;)) + labs(title = &quot;Bike Choice: Casuals vs. Members&quot;, subtitle = &quot;Days of the Week&quot;, x = &quot;&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.5, &quot;cm&quot;), legend.text = element_text(size = 10), plot.title = element_text(color = &quot;black&quot;, size = 22)) Figure 3.14: Casual vs.&#xA0;Member - Choice per Day # Bar chart: bike type choice member vs. casual, by month bike_data %&gt;% select(trip_month, rideable_type, member_casual) %&gt;% group_by(trip_month, rideable_type, member_casual) %&gt;% count(rideable_type) %&gt;% ggplot(aes(x = member_casual, y = n, fill = rideable_type)) + geom_col() + facet_wrap(~ trip_month) + scale_y_continuous(labels = comma) + scale_fill_manual(values = c(&quot;#E5C494&quot;, &quot;#B3B3B3&quot;, &quot;#FFD92F&quot;), labels = c(&quot;Classic&quot;, &quot;Docked&quot;, &quot;Electric&quot;)) + labs(title = &quot;Bike Choice: Casuals vs. Members&quot;, subtitle = &quot;Months of the Year&quot;, x = &quot;&quot;, y = &quot;Number of Rides\\n&quot;) + theme(legend.title = element_blank(), legend.key.size = unit(0.5, &quot;cm&quot;), legend.text = element_text(size = 10), plot.title = element_text(size = 22)) Figure 3.15: Casual vs.&#xA0;Member - Choice per Month 3.4.4 Viz: Popular Routes # Top 10 routes member vs. casual popular_routes &lt;- bike_data %&gt;% select(route, member_casual) %&gt;% group_by(route, member_casual) %&gt;% count(route) %&gt;% arrange(member_casual, desc(n)) %&gt;% group_by(member_casual) %&gt;% slice(1:10) # Bar Chart: Top 10 routes casual popular_routes %&gt;% filter(member_casual == &quot;Casual&quot;) %&gt;% mutate(order = rank(route)) %&gt;% ggplot(aes(x = reorder(route, n), y = n)) + geom_col(aes(fill = reorder(route, -n)), position = &quot;dodge&quot;) + scale_y_continuous(labels = comma) + geom_text(aes(y = 1200, label = n), size = 3) + labs(title = &quot;Top 10 Popular Routes for Casual Riders\\n&quot;, x = &quot;&quot;, y = &quot;Number of Rides&quot;) + theme(panel.background = element_blank(), legend.title = element_text(size = 10), plot.title = element_text(size = 22, hjust = 1.2), legend.direction = &quot;vertical&quot;, legend.text = element_text(size = 8), legend.key.size = unit(0.5, &quot;cm&quot;), legend.position = &quot;none&quot;) + coord_flip() + scale_fill_manual(name = &quot;Route Name&quot;, values = colorRampPalette(brewer.pal(8,&quot;Set2&quot;))(10)) Figure 3.16: # Pie Chart: Top 10 routes casual popular_routes %&gt;% filter(member_casual == &quot;Casual&quot;) %&gt;% mutate(perc = percent(n/sum(n), accuracy = 0.1)) %&gt;% ggplot(aes(x = 1, y = n, fill = reorder(route, -n))) + geom_col(color = &quot;black&quot;, size = 0.2, position = &quot;stack&quot;, orientation = &quot;x&quot;) + geom_text(aes(x = 1.25, label = perc), position = position_stack(vjust = 0.5), size = 2.5) + coord_polar(theta = &quot;y&quot;, direction = -1) + theme_void() + labs(title = &quot;Top 10 Popular Routes for Casual Riders\\n&quot;, x = &quot;&quot;, y = &quot;Number of Rides&quot;) + theme(plot.title = element_text(size = 22, hjust = -0.25), legend.title = element_blank(), legend.text = element_text(size = 8), legend.key.size = unit(0.45, &quot;cm&quot;)) + scale_fill_manual(name = &quot;Route Name&quot;, values = colorRampPalette(brewer.pal(8,&quot;Set2&quot;))(10)) Figure 3.17: # Bar Chart: Top 10 routes member popular_routes %&gt;% filter(member_casual == &quot;Member&quot;) %&gt;% mutate(order = rank(route)) %&gt;% ggplot(aes(x = reorder(route, n), y = n)) + geom_col(aes(fill = reorder(route, -n)), position = &quot;dodge&quot;) + scale_y_continuous(labels = comma) + geom_text(aes(y = 375, label = n), size = 3) + labs(title = &quot;Top 10 Popular Routes for Annual Members\\n&quot;, x = &quot;&quot;, y = &quot;Number of Rides&quot;) + theme(panel.background = element_blank(), legend.title = element_text(size = 12), plot.title = element_text(size = 22, hjust = 1.25), legend.text = element_text(size = 10), legend.position = &quot;none&quot;, legend.key.size = unit(0.75, &quot;cm&quot;)) + coord_flip() + scale_fill_brewer(name = &quot;Route Name&quot;, palette = &quot;Set3&quot;) Figure 3.18: # Pie Chart: Top 10 routes member popular_routes %&gt;% filter(member_casual == &quot;Member&quot;) %&gt;% mutate(perc = percent(n/sum(n), accuracy = 0.1)) %&gt;% ggplot(aes(x = 1, y = n, fill = reorder(route, -n))) + geom_col(color = &quot;black&quot;, size = 0.2, position = &quot;stack&quot;, orientation = &quot;x&quot;) + geom_text(aes(x = 1.25, label = perc), position = position_stack(vjust = 0.5), size = 3) + coord_polar(theta = &quot;y&quot;, direction = -1) + theme_void() + labs(title = &quot;Top 10 Popular Routes for Annual Members\\n&quot;, x = &quot;&quot;, y = &quot;Number of Rides&quot;) + theme(plot.title = element_text(size = 22, hjust = -0.25), legend.title = element_blank(), legend.text = element_text(size = 10), legend.key.size = unit(0.65, &quot;cm&quot;)) + scale_fill_brewer(palette = &quot;Set3&quot;) Figure 3.19: # Join geo data popular_routes &lt;- popular_routes %&gt;% inner_join(bike_data %&gt;% select(route, start_lat:end_lng, member_casual), by = c(&quot;member_casual&quot;, &quot;route&quot;)) %&gt;% distinct(route, .keep_all = TRUE) %&gt;% group_by(member_casual) %&gt;% mutate(rank = order(order(n, decreasing=TRUE))) # Import map of Chicago chicago &lt;- get_stamenmap(bbox = c(left = -87.75, bottom = 41.7, right = -87.5, top = 41.95), zoom = 11) casual_map &lt;- ggmap(chicago, darken = 0.5) + stat_density2d(data = filter(popular_routes, member_casual == &quot;Casual&quot;), aes(x = start_lng, y = start_lat, fill = ..level.., alpha = ..level..), geom = &quot;polygon&quot;, bins = 10) + stat_density2d(data = filter(popular_routes, member_casual == &quot;Casual&quot;), aes(x = end_lng, y = end_lat, fill = ..level.., alpha = ..level..), geom = &quot;polygon&quot;, bins = 10) + scale_fill_gradient(low = &quot;green&quot;, high = &quot;red&quot;) + scale_alpha(range = c(0, 0.75), guide = &quot;none&quot;) + labs(title = &quot;Casual Rider Hotspot&quot;, x = &quot;&quot;, y = &quot;&quot;) + theme(legend.position = &quot;none&quot;, plot.title = element_text(size = 16), axis.ticks = element_blank(), axis.text = element_blank()) member_map &lt;- ggmap(chicago, darken = 0.5) + stat_density2d(data = filter(popular_routes, member_casual == &quot;Member&quot;), aes(x = start_lng, y = start_lat, fill = ..level.., alpha = ..level..), geom = &quot;polygon&quot;, bins = 10) + stat_density2d(data = filter(popular_routes, member_casual == &quot;Member&quot;), aes(x = end_lng, y = end_lat, fill = ..level.., alpha = ..level..), geom = &quot;polygon&quot;, bins = 10) + scale_fill_gradient(low = &quot;green&quot;, high = &quot;red&quot;) + scale_alpha(range = c(0, 0.75), guide = &quot;none&quot;) + labs(title = &quot;Annual Member Hotspot&quot;, x = &quot;&quot;, y = &quot;&quot;) + theme(legend.position = &quot;none&quot;, plot.title = element_text(size = 16), axis.ticks = element_blank(), axis.text = element_blank()) grid.arrange(casual_map, member_map, nrow = 1) Figure 3.20: Heatmaps Based on Popular Routes 3.5 Share &amp; Act (Final Report) 3.5.1 Key Observations Figure 3.1 shows that the target group &#x2013; casual riders &#x2013; is 42% of total users. Figures 3.2 and 3.3 show that 54.7% of total casual rides are taken during the weekend. Offer a membership program tailored towards discounted weekend use with incentive(s) to upgrade to an annual membership. Figures 3.4 and 3.5 show that almost half of the total casual rides are taken during the summer. Offer a summer membership program with incentive(s) to upgrade to an annual membership. Figures 3.6 and 3.7 show that casual riders tend to take trips during summer afternoons, suggesting that the target demographic is not likely to be students or commuters. Students and commuters would show higher use during off-seasons, as well as mornings and weekdays - as is shown by members in Figures 3.6 and 3.8 Figure 3.20 shows that casual riders take the most trips in Downtown Chicago (see Figure 3.16 for popular route names). Allocate more resources towards marketing in Downtown Chicago (The Loop). Figures relevant to bike choice (3.11 - 3.15) indicate that casual riders and annual members exhibit similar behavior; however, it&#x2019;s worth investigating why 12% of casual riders use a docked bike when 0% of members do. 3.5.2 Actionables Allocate more resources towards marketing in Downtown Chicago (The Loop) Offer a membership program tailored towards discounted weekend use Offer a summer membership program Equip both programs with incentive(s) to upgrade to an annual membership References "],["rsql.html", "Chapter 4 Visualizing Sentiment [DA|R] 4.1 Description 4.2 Setup 4.3 Create SQL Database 4.4 Write &amp; Query Table 4.5 Analysis 4.6 Alter &amp; Update Table 4.7 R &amp; SQL", " Chapter 4 Visualizing Sentiment [DA|R] Database queries and viz using SQL and the R package dbplyr (Wickham, Girlich, and Ruiz 2022). 4.1 Description I came across this cool dataset on Kaggle about sentiment analysis using old tweets from 2009; this project demonstrates the rationale behind using SQLite. Motivation: Showing how I can access or create databases and query them, which allows me to do all the similar viz and analysis done in my Google Capstone project. This process allows for multifaceted and streamlined analysis that can be adapted to professional contexts with local databases I.e., using SQL and R in tandem to filter/work with any database by reference until the data&#x2019;s satisfactory or small enough to collect and work with locally for viz 4.2 Setup Boilerplate code for the R packages that enable the link between R and SQL: packages &lt;- c(&quot;tidyverse&quot;, &quot;lubridate&quot;, &quot;tidytext&quot;, &quot;dbplyr&quot;, &quot;RSQLite&quot;, &quot;RColorBrewer&quot;, &quot;wordcloud&quot;) for (package in packages) { if (!require(package, character.only = TRUE)) { print(&quot;Installing package(s)&quot;) install.packages(package, repos = &quot;http://cran.us.r-project.org&quot;) library(package, character.only = TRUE) if (require(package, character.only = TRUE)) { print(&quot;Package(s) installed and loaded&quot;) } else { stop(&quot;Could not install package(s)&quot;) } } } 4.3 Create SQL Database For this project I create a local database from the sentiment-140.csv file with the SQLite database backend. tweets &lt;- read_csv(&quot;sentiment-140.csv&quot;, col_names=c(&quot;target&quot;, &quot;id&quot;, &quot;date&quot;, &quot;flag&quot;, &quot;user&quot;, &quot;text&quot;), show_col_types=FALSE) %&gt;% glimpse() ## Rows: 1,600,000 ## Columns: 6 ## $ target &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, &#x2026; ## $ id &lt;dbl&gt; 1467810369, 1467810672, 1467810917, 1467811184, 1467811193, 146&#x2026; ## $ date &lt;chr&gt; &quot;Mon Apr 06 22:19:45 PDT 2009&quot;, &quot;Mon Apr 06 22:19:49 PDT 2009&quot;,&#x2026; ## $ flag &lt;chr&gt; &quot;NO_QUERY&quot;, &quot;NO_QUERY&quot;, &quot;NO_QUERY&quot;, &quot;NO_QUERY&quot;, &quot;NO_QUERY&quot;, &quot;NO&#x2026; ## $ user &lt;chr&gt; &quot;_TheSpecialOne_&quot;, &quot;scotthamilton&quot;, &quot;mattycus&quot;, &quot;ElleCTF&quot;, &quot;Kar&#x2026; ## $ text &lt;chr&gt; &quot;@switchfoot http://twitpic.com/2y1zl - Awww, that&apos;s a bummer. &#x2026; The special string &quot;:memory:&quot; causes SQLite to make a temporary in-memory database: # Create &amp; connect to database db &lt;- dbConnect(SQLite(), &quot;:memory:&quot;) Aside For databases hosted on a server, the code would look something like this: db &lt;- DBI::dbConnect(RMariaDB::MariaDB(), host = &quot;database.rstudio.com&quot;, user = &quot;hadley&quot;, password = rstudioapi::askForPassword(&quot;Database password&quot;)) With the backend being either MariaDB, Postgres, ODBC, or BigQuery &#x2013; using RMariaDB::MariaDB(), RPostgres::Postgres(), odbc::odbc(), and bigrquery::bigquery() respectively. 4.4 Write &amp; Query Table Now that the connection to the database is established, add the table and perform sanity checks: # Write data copy_to(db, tweets) dbListFields(db, &quot;tweets&quot;) ## [1] &quot;target&quot; &quot;id&quot; &quot;date&quot; &quot;flag&quot; &quot;user&quot; &quot;text&quot; head(dbReadTable(db, &quot;tweets&quot;)) ## target id date flag user ## 1 0 1467810369 Mon Apr 06 22:19:45 PDT 2009 NO_QUERY _TheSpecialOne_ ## 2 0 1467810672 Mon Apr 06 22:19:49 PDT 2009 NO_QUERY scotthamilton ## 3 0 1467810917 Mon Apr 06 22:19:53 PDT 2009 NO_QUERY mattycus ## 4 0 1467811184 Mon Apr 06 22:19:57 PDT 2009 NO_QUERY ElleCTF ## 5 0 1467811193 Mon Apr 06 22:19:57 PDT 2009 NO_QUERY Karoli ## 6 0 1467811372 Mon Apr 06 22:20:00 PDT 2009 NO_QUERY joy_wolf ## text ## 1 @switchfoot http://twitpic.com/2y1zl - Awww, that&apos;s a bummer. You shoulda got David Carr of Third Day to do it. ;D ## 2 is upset that he can&apos;t update his Facebook by texting it... and might cry as a result School today also. Blah! ## 3 @Kenichan I dived many times for the ball. Managed to save 50% The rest go out of bounds ## 4 my whole body feels itchy and like its on fire ## 5 @nationwideclass no, it&apos;s not behaving at all. i&apos;m mad. why am i here? because I can&apos;t see you all over there. ## 6 @Kwesidei not the whole crew As pulled from its description, this data contains the following 6 fields: target: The polarity of the tweet (0 = negative, 4 = positive) id: The id of the tweet (2087) date: The date of the tweet (Sat May 16 23:58:44 UTC 2009) flag: The query (lyx). If there is no query, then this value is NO_QUERY. user: The user that tweeted (robotickilldozr) text: The text of the tweet (Lyx is cool) Now I can also use SQL commands in this document to query the database: SELECT * FROM tweets LIMIT 2 Table 4.1: 2 records target id date flag user text 0 1467810369 Mon Apr 06 22:19:45 PDT 2009 NO_QUERY TheSpecialOne (switchfoot?) http://twitpic.com/2y1zl - Awww, that&#x2019;s a bummer. You shoulda got David Carr of Third Day to do it. ;D 0 1467810672 Mon Apr 06 22:19:49 PDT 2009 NO_QUERY scotthamilton is upset that he can&#x2019;t update his Facebook by texting it&#x2026; and might cry as a result School today also. Blah! And can also store its output as a dataframe in R if needed: # Stored as &quot;glimpse&quot; str(glimpse) ## &apos;data.frame&apos;: 2 obs. of 6 variables: ## $ target: num 0 0 ## $ id : num 1467810369 1467810672 ## $ date : chr &quot;Mon Apr 06 22:19:45 PDT 2009&quot; &quot;Mon Apr 06 22:19:49 PDT 2009&quot; ## $ flag : chr &quot;NO_QUERY&quot; &quot;NO_QUERY&quot; ## $ user : chr &quot;_TheSpecialOne_&quot; &quot;scotthamilton&quot; ## $ text : chr &quot;@switchfoot http://twitpic.com/2y1zl - Awww, that&apos;s a bummer. You shoulda got David Carr of Third Day to do it. ;D&quot; &quot;is upset that he can&apos;t update his Facebook by texting it... and might cry as a result School today also. Blah!&quot; -- removing user tweetpet that only tweets &quot;Clean me!&quot; DELETE FROM tweets WHERE user LIKE &apos;tweetpet&apos; -- number of users SELECT COUNT(DISTINCT user) AS user_count FROM tweets Table 4.2: 1 records user_count 659774 -- counting number of each sentiment SELECT target, COUNT(target) AS target_count FROM tweets GROUP BY target Table 4.3: 2 records target target_count 0 799690 4 800000 -- top 10 users SELECT user, COUNT(user) AS tweet_count FROM tweets GROUP BY user ORDER BY tweet_count DESC LIMIT 10 Table 4.4: Displaying records 1 - 10 user tweet_count lost_dog 549 webwoke 345 SallytheShizzle 281 VioletsCRUK 279 mcraddictal 276 tsarnick 248 what_bugs_u 246 Karen230683 238 DarkPiano 236 SongoftheOss 227 -- 5 tweets from the top 10 users SELECT target, user, text, date FROM tweets WHERE user IN (SELECT user FROM (SELECT user, COUNT(user) AS tweet_count FROM tweets GROUP BY user ORDER BY tweet_count DESC LIMIT 10)) GROUP BY user LIMIT 5 Table 4.5: 5 records target user text date 0 DarkPiano (aliholden?) Not good, I hate that! Sat May 02 04:59:54 PDT 2009 0 Karen230683 (KingKiwi?) i want to be somewhere with no rain its not very nice here today Tue Apr 07 01:32:50 PDT 2009 0 SallytheShizzle (OfficialAS?) without ASA&#x2026;.i&#x2019;m sorry if you didn&#x2019;t know that already Fri May 22 03:10:00 PDT 2009 0 SongoftheOss Still upset at not being a lottery winner again&#x2026;no fair Sat May 02 07:56:42 PDT 2009 0 VioletsCRUK (jason_2008?) Hello! Im out of coffee this morning..that&#x2019;s what i get for winding you up yesterday!!! Lol Have a good day! Tue Apr 07 01:20:58 PDT 2009 -- sentiment counts from the top 5 users SELECT user, target, COUNT(target) AS target_count FROM tweets WHERE user IN (SELECT user FROM (SELECT user, COUNT(user) AS tweet_count FROM tweets GROUP BY user ORDER BY tweet_count DESC LIMIT 5)) GROUP BY user, target Table 4.6: 9 records user target target_count SallytheShizzle 0 183 SallytheShizzle 4 98 VioletsCRUK 0 61 VioletsCRUK 4 218 lost_dog 0 549 mcraddictal 0 210 mcraddictal 4 66 webwoke 0 264 webwoke 4 81 -- 5 positive tweets from top users SELECT target, user, COUNT(user) AS tweet_count, text, date FROM tweets WHERE target = 4 GROUP BY user ORDER BY tweet_count DESC LIMIT 5 Table 4.7: 5 records target user tweet_count text date 4 what_bugs_u 246 (JadeMcCray?) why limit your story to 140 chr. Tell all what is annoying you at www.iamsoannoyed.com ,it will help relieve your stress Sat May 30 20:18:46 PDT 2009 4 DarkPiano 231 (suesshirtshop?) Same to you, Sue! Tue Apr 07 04:19:32 PDT 2009 4 VioletsCRUK 218 Morning all! Hope you have a good one Tue Apr 07 01:14:14 PDT 2009 4 tsarnick 212 (RumLover?) if more than one lady agrees to attend with with you then I am fine with it! Mon Apr 06 22:52:56 PDT 2009 4 keza34 211 (ComedyQueen?) what a fibber.dont choke when your halo slips.lol Sun Apr 19 06:39:32 PDT 2009 -- 5 negative tweets from top users SELECT target, user, COUNT(user) AS tweet_count, text, date FROM tweets WHERE target = 0 GROUP BY user ORDER BY tweet_count DESC LIMIT 5 Table 4.8: 5 records target user tweet_count text date 0 lost_dog 549 (NyleW?) I am lost. Please help me find a good home. Fri May 01 22:54:02 PDT 2009 0 webwoke 264 auchh, drop by 1 (32)elitestv.com Fri Jun 05 14:12:05 PDT 2009 0 wowlew 210 isPlayer Has Died! Sorry Thu May 14 04:02:33 PDT 2009 0 mcraddictal 210 (mcr_chick?) i wanna sleep.lol. Urgh. I feel like crying Sat May 09 19:50:38 PDT 2009 0 SallytheShizzle 183 (OfficialAS?) without ASA&#x2026;.i&#x2019;m sorry if you didn&#x2019;t know that already Fri May 22 03:10:00 PDT 2009 4.5 Analysis I use regex in R to clean the tweets in order to properly tokenize the words within. clean_tweet &lt;- function(x) { x %&gt;% str_remove_all(&quot; ?(f|ht)(tp)(s?)(://)([^\\\\.]*)[\\\\.|/](\\\\S*)&quot;) %&gt;% str_remove_all(&quot;@[[:alnum:]_]{4,}&quot;) %&gt;% str_remove_all(&quot;#[[:alnum:]_]+&quot;) %&gt;% str_replace_all(&quot;&amp;amp;&quot;, &quot;and&quot;) %&gt;% str_remove_all(&quot;[[:punct:]]&quot;) %&gt;% str_replace_all(&quot;[[:digit:]]&quot;, &quot;&quot;) %&gt;% str_replace_all(&quot;([[:alpha:]])\\\\1{2,}&quot;, &quot;\\\\1&quot;) %&gt;% str_replace_all(&quot; *\\\\b[[:alpha:]]{1,2}\\\\b *&quot;, &quot; &quot;) %&gt;% str_remove_all(&quot;^RT:? &quot;) %&gt;% str_replace_all(&quot;\\\\\\n&quot;, &quot; &quot;) %&gt;% str_to_lower() %&gt;% str_trim(&quot;both&quot;) %&gt;% str_replace_all(&quot;\\\\s+&quot;, &quot; &quot;) } 4.5.1 Sentiment Word Clouds -- negative tweets SELECT text, target FROM tweets WHERE target = 0 glimpse(tweets_neg) ## Rows: 799,690 ## Columns: 2 ## $ text &lt;chr&gt; &quot;@switchfoot http://twitpic.com/2y1zl - Awww, that&apos;s a bummer. &#x2026; ## $ target &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, &#x2026; words_neg &lt;- tweets_neg %&gt;% mutate(text = clean_tweet(text)) %&gt;% unnest_tokens(word,text) %&gt;% anti_join(stop_words) %&gt;% count() %&gt;% arrange(desc(freq)) words_neg %&gt;% glimpse() %&gt;% with(wordcloud(word, freq, min.freq=2500, random.order=FALSE, rot.per=0, colors=c(&quot;#FC9272&quot;, &quot;#FB6A4A&quot;, &quot;#EF3B2C&quot;, &quot;#CB181D&quot;, &quot;#A50F15&quot;))) ## Rows: 225,979 ## Columns: 3 ## $ target &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, &#x2026; ## $ word &lt;chr&gt; &quot;dont&quot;, &quot;day&quot;, &quot;miss&quot;, &quot;sad&quot;, &quot;time&quot;, &quot;home&quot;, &quot;lol&quot;, &quot;feel&quot;, &quot;b&#x2026; ## $ freq &lt;int&gt; 44873, 39665, 30512, 27397, 26831, 23962, 21864, 21810, 21605, &#x2026; Figure 4.1: Negative Sentiment Word Cloud -- negative tweets SELECT text, target FROM tweets WHERE target = 4 words_pos &lt;- tweets_pos %&gt;% mutate(text = clean_tweet(text)) %&gt;% unnest_tokens(word,text) %&gt;% anti_join(stop_words) %&gt;% count() %&gt;% arrange(desc(freq)) words_pos %&gt;% glimpse() %&gt;% with(wordcloud(word, freq, min.freq=2500, random.order=FALSE, rot.per=0, colors=c(&quot;#A1D99B&quot;, &quot;#74C476&quot;, &quot;#41AB5D&quot;, &quot;#238B45&quot;, &quot;#006D2C&quot;))) ## Rows: 251,424 ## Columns: 3 ## $ target &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, &#x2026; ## $ word &lt;chr&gt; &quot;love&quot;, &quot;day&quot;, &quot;lol&quot;, &quot;time&quot;, &quot;night&quot;, &quot;dont&quot;, &quot;haha&quot;, &quot;happy&quot;,&#x2026; ## $ freq &lt;int&gt; 47826, 46006, 33725, 29432, 22350, 22008, 20863, 20178, 20156, &#x2026; Figure 4.2: Positive Sentiment Word Cloud # Join data words &lt;- words_pos %&gt;% inner_join(words_neg, by=&quot;word&quot;, suffix= c(&quot;_pos&quot;, &quot;_neg&quot;)) %&gt;% spread(target_pos, freq_pos) %&gt;% spread(target_neg, freq_neg) glimpse(words) ## Rows: 86,779 ## Columns: 3 ## $ word &lt;chr&gt; &quot;aaa&quot;, &quot;aaah&quot;, &quot;aaahh&quot;, &quot;aaand&quot;, &quot;aaargh&quot;, &quot;aaaw&quot;, &quot;aaaww&quot;, &quot;aabt&#x2026; ## $ `4` &lt;int&gt; 2, 47, 6, 4, 5, 16, 6, 1, 6, 5, 1, 4, 2, 2, 7, 247, 1, 1, 7, 3, 1&#x2026; ## $ `0` &lt;int&gt; 4, 62, 3, 10, 23, 23, 6, 1, 3, 1, 2, 2, 2, 3, 20, 260, 4, 1, 2, 1&#x2026; # Make data frame of pos and neg sentiment frequencies with words as indices rownames(words) &lt;- words[,&apos;word&apos;] words[,&apos;word&apos;] &lt;- NULL colnames(words) &lt;- c(&quot;positive&quot;, &quot;negative&quot;) glimpse(words) ## Rows: 86,779 ## Columns: 2 ## $ positive &lt;int&gt; 2, 47, 6, 4, 5, 16, 6, 1, 6, 5, 1, 4, 2, 2, 7, 247, 1, 1, 7, &#x2026; ## $ negative &lt;int&gt; 4, 62, 3, 10, 23, 23, 6, 1, 3, 1, 2, 2, 2, 3, 20, 260, 4, 1, &#x2026; comparison.cloud(words, max.words=225, random.order=FALSE, rot.per=0, title.colors=c(&quot;#00441B&quot;, &quot;#67000D&quot;), colors=c(&quot;#006D2C&quot;, &quot;#A50F15&quot;), title.bg.colors=c(&quot;#E5F5E0&quot;, &quot;#FEE0D2&quot;)) Figure 4.3: Comparison Cloud A comparison cloud provides a supplemental view of the sentiments. For example, the word &#x201C;day&#x201D; is most frequent in both positive and negative word clouds, with the comparison cloud revealing that it&#x2019;s more often in a positive context. 4.6 Alter &amp; Update Table Converting the dates to a standard format and extracting the days: ALTER TABLE tweets ADD COLUMN day TEXT; UPDATE tweets SET day = SUBSTR(date, 1, 3); ALTER TABLE tweets ADD COLUMN datetime TEXT; UPDATE tweets SET datetime = SUBSTR(date, 5, 7) || &quot; 2009 &quot; || SUBSTR(date, 12, 12); -- Splitting date into day and date-time SELECT date, day, datetime FROM tweets Table 4.9: Displaying records 1 - 10 date day datetime Mon Apr 06 22:19:45 PDT 2009 Mon Apr 06 2009 22:19:45 PDT Mon Apr 06 22:19:49 PDT 2009 Mon Apr 06 2009 22:19:49 PDT Mon Apr 06 22:19:53 PDT 2009 Mon Apr 06 2009 22:19:53 PDT Mon Apr 06 22:19:57 PDT 2009 Mon Apr 06 2009 22:19:57 PDT Mon Apr 06 22:19:57 PDT 2009 Mon Apr 06 2009 22:19:57 PDT Mon Apr 06 22:20:00 PDT 2009 Mon Apr 06 2009 22:20:00 PDT Mon Apr 06 22:20:03 PDT 2009 Mon Apr 06 2009 22:20:03 PDT Mon Apr 06 22:20:03 PDT 2009 Mon Apr 06 2009 22:20:03 PDT Mon Apr 06 22:20:05 PDT 2009 Mon Apr 06 2009 22:20:05 PDT Mon Apr 06 22:20:09 PDT 2009 Mon Apr 06 2009 22:20:09 PDT ALTER TABLE tweets DROP COLUMN date; 4.7 R &amp; SQL Using the dbplyr package (Wickham, Girlich, and Ruiz 2022) allows for a &#x201C;weaving&#x201D; of R and SQL. My favorite part of all this: The most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database on the remote server, not in R on your local machine. When working with databases, dplyr tries to be as lazy as possible: It never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step. The tweets table in our database can be referenced and that reference is stored in R: tweets_db &lt;- tbl(db, &quot;tweets&quot;) tweets_db ## # Source: table&lt;tweets&gt; [?? x 7] ## # Database: sqlite 3.39.3 [:memory:] ## target id flag user text day datet&#x2026;&#xB9; ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0 1467810369 NO_QUERY _TheSpecialOne_ @switchfoot http://&#x2026; Mon Apr 06&#x2026; ## 2 0 1467810672 NO_QUERY scotthamilton is upset that he ca&#x2026; Mon Apr 06&#x2026; ## 3 0 1467810917 NO_QUERY mattycus @Kenichan I dived m&#x2026; Mon Apr 06&#x2026; ## 4 0 1467811184 NO_QUERY ElleCTF my whole body feels&#x2026; Mon Apr 06&#x2026; ## 5 0 1467811193 NO_QUERY Karoli @nationwideclass no&#x2026; Mon Apr 06&#x2026; ## 6 0 1467811372 NO_QUERY joy_wolf @Kwesidei not the w&#x2026; Mon Apr 06&#x2026; ## 7 0 1467811592 NO_QUERY mybirch Need a hug Mon Apr 06&#x2026; ## 8 0 1467811594 NO_QUERY coZZ @LOLTrish hey long&#x2026; Mon Apr 06&#x2026; ## 9 0 1467811795 NO_QUERY 2Hood4Hollywood @Tatiana_K nope the&#x2026; Mon Apr 06&#x2026; ## 10 0 1467812025 NO_QUERY mimismo @twittera que me mu&#x2026; Mon Apr 06&#x2026; ## # &#x2026; with more rows, and abbreviated variable name &#xB9;&#x200B;datetime Notice how it&#x2019;s a remote source in the database and not a dataframe. Using collect() will store the query results into a dataframe (tibble), which then allows for us to play with it in R. For the following visuals I collect the sentiment means per month, day, and hour. The dbplot package (Ruiz 2020) is great for quickly visualizing data without collecting, but is naturally more limited. # Average sentiment per month avg_month &lt;- tweets_db %&gt;% select(target, datetime) %&gt;% mutate(month = substr(datetime, 1, 3)) %&gt;% group_by(month) %&gt;% summarise( sentiment = mean(target, na.rm=TRUE), n = n() ) %&gt;% arrange(desc(sentiment)) %&gt;% print() %&gt;% show_query() %&gt;% collect() ## # Source: SQL [3 x 3] ## # Database: sqlite 3.39.3 [:memory:] ## # Ordered by: desc(sentiment) ## month sentiment n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 May 2.44 576189 ## 2 Apr 2.34 99893 ## 3 Jun 1.69 923608 ## &lt;SQL&gt; ## SELECT `month`, AVG(`target`) AS `sentiment`, COUNT(*) AS `n` ## FROM ( ## SELECT `target`, `datetime`, SUBSTR(`datetime`, 1, 3) AS `month` ## FROM `tweets` ## ) ## GROUP BY `month` ## ORDER BY `sentiment` DESC avg_month &lt;- avg_month %&gt;% mutate(month = fct_case_when(month == &apos;Apr&apos; ~ &apos;Apr&apos;, month == &apos;May&apos; ~ &apos;May&apos;, month == &apos;Jun&apos; ~ &apos;Jun&apos;)) # Centered about the true neutral (2) avg_month %&gt;% ggplot(aes(x = month, y = sentiment - 2, fill = sentiment &gt; 2)) + geom_col() + geom_hline(yintercept = 0) + labs(title = &quot;Sentiments Centered About the True Neutral&quot;, x = &quot;\\nMonth&quot;, y = &quot;Average Sentiment\\n&quot;) + scale_fill_manual(values = c(&quot;#67000D&quot;, &quot;#00441B&quot;)) + theme_minimal() + theme(legend.position = &quot;none&quot;) # Pie chart avg_month %&gt;% mutate(perc = percent(n/sum(n))) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = month)) + geom_bar(stat = &quot;identity&quot;, size = 0.25, color = &quot;black&quot;, position = &quot;fill&quot;) + coord_polar(theta = &quot;y&quot;, direction = -1) + geom_text(aes(x = 1.125, label = perc), position = position_fill(vjust=0.5), size = 4.5) + theme_void() + labs(title = &quot;Sample Group: Amount of Tweets by Month&quot;) + scale_fill_manual(values = c(&quot;#BEBADA&quot;, &quot;#8DD3C7&quot;, &quot;#FDB462&quot;)) + theme(legend.title = element_blank(), legend.text = element_text(size = 12), legend.key.size = unit(0.65, &quot;cm&quot;), plot.title = element_text(color = &quot;black&quot;, size = 22, hjust = 0.5)) # Average sentiment per day avg_day &lt;- tweets_db %&gt;% select(target, day) %&gt;% group_by(day) %&gt;% summarise( sentiment = mean(target, na.rm=TRUE), n = n() ) %&gt;% arrange(desc(sentiment)) %&gt;% print() %&gt;% show_query() %&gt;% collect() ## # Source: SQL [7 x 3] ## # Database: sqlite 3.39.3 [:memory:] ## # Ordered by: desc(sentiment) ## day sentiment n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Sun 2.31 344533 ## 2 Mon 2.29 310176 ## 3 Sat 2.09 330831 ## 4 Fri 1.97 225524 ## 5 Tue 1.83 185785 ## 6 Wed 1.17 96806 ## 7 Thu 0.977 106035 ## &lt;SQL&gt; ## SELECT `day`, AVG(`target`) AS `sentiment`, COUNT(*) AS `n` ## FROM ( ## SELECT `target`, `day` ## FROM `tweets` ## ) ## GROUP BY `day` ## ORDER BY `sentiment` DESC # Correctly displays days of the week in order, # and centers it about the true neutral (2) avg_day &lt;- avg_day %&gt;% mutate(day = fct_case_when(day == &apos;Mon&apos; ~ &apos;Mon&apos;, day == &apos;Tue&apos; ~ &apos;Tue&apos;, day == &apos;Wed&apos; ~ &apos;Wed&apos;, day == &apos;Thu&apos; ~ &apos;Thu&apos;, day == &apos;Fri&apos; ~ &apos;Fri&apos;, day == &apos;Sat&apos; ~ &apos;Sat&apos;, day == &apos;Sun&apos; ~ &apos;Sun&apos;)) avg_day %&gt;% ggplot(aes(x = day, y = sentiment - 2, fill = sentiment &gt; 2)) + geom_col() + geom_hline(yintercept = 0) + labs(title = &quot;Sentiments Centered About the True Neutral&quot;, x = &quot;\\nDay&quot;, y = &quot;Average Sentiment\\n&quot;) + scale_fill_manual(values = c(&quot;#67000D&quot;, &quot;#00441B&quot;)) + theme_minimal() + theme(legend.position = &quot;none&quot;) # Bar chart avg_day %&gt;% ggplot(aes(x = day, y = n)) + geom_col() + labs(title = &quot;Sample Group: Amount of Tweets by Day&quot;, x = &quot;\\nDay&quot;, y = &quot;Number of Tweets\\n&quot;) + theme_minimal() # Average sentiment per hour avg_hour &lt;- tweets_db %&gt;% select(target, datetime) %&gt;% mutate(hour = substr(datetime, 14, 15)) %&gt;% group_by(hour) %&gt;% summarise( sentiment = mean(target, na.rm=TRUE), n = n() ) %&gt;% print() %&gt;% show_query() %&gt;% collect() ## # Source: SQL [?? x 3] ## # Database: sqlite 3.39.3 [:memory:] ## hour sentiment n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 00 2.24 80865 ## 2 01 2.37 75268 ## 3 02 2.37 73885 ## 4 03 2.29 74166 ## 5 04 2.18 76988 ## 6 05 2.08 78623 ## 7 06 2.02 80846 ## 8 07 1.98 83654 ## 9 08 1.89 76270 ## 10 09 1.83 67278 ## # &#x2026; with more rows ## &lt;SQL&gt; ## SELECT `hour`, AVG(`target`) AS `sentiment`, COUNT(*) AS `n` ## FROM ( ## SELECT `target`, `datetime`, SUBSTR(`datetime`, 14, 2) AS `hour` ## FROM `tweets` ## ) ## GROUP BY `hour` # Sentiment over the course of a day (on average) avg_hour %&gt;% ggplot(aes(x = hour, y = sentiment, group=1)) + geom_line() + geom_point() + geom_line(aes(y = 2), color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_line(aes(y = mean(sentiment)), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + labs(x = &quot;\\nHour&quot;, y = &quot;Average Sentiment\\n&quot;) + theme_minimal() The sample mean (red line) is slightly below the true neutral (blue line). # Closing the connection dbDisconnect(db) References "],["nn-xor.html", "Chapter 5 NN: XOR [ML|PY,MATH] 5.1 Backpropagation and XOR 5.2 Activation 5.3 Multivariable Calculus 5.4 Conclusion 5.5 Learning XOR in TensorFlow", " Chapter 5 NN: XOR [ML|PY,MATH] Under the Hood: Backpropagation 5.1 Backpropagation and XOR For my Multivariable Calculus final project, I chose to study the backpropagation algorithm in the context of a fully-connected feed-forward neural network with one hidden layer &#x2013; the canonical multilayer perceptron or &#x201C;vanilla&#x201D; neural net. In short, backpropagation is a machine learning technique that uses gradient descent calculus and grants neural networks the ability to perform non-linear classification tasks. I will use this learning technique to show how such a neural network is capable of learning the XOR logical gate. 5.1.1 Motivation The XOR classification task is non-linear (not linearly separable) because it&#x2019;s quite literally impossible to use a single line to separate our two groups. In the case of XOR, the gate outputs a 1 if and only if one of the two inputs is a 1, and 0 otherwise. Figure 5.1: Linear vs.&#xA0;Nonlinear Classification (Google Images) Referencing the right side of Figure 5.1 above, the two groups we have to classify for this case are: When it should output a 1 (labeled as a green dot) When it should output a 0 (labeled as a red dot) If the XOR classification task were linear, we would be able to separate the green dots from the red dots with a single line (as we do with OR). Aside As a side note, focusing on the XOR case allows us to ignore the confusing notation that naturally comes with the generalized form of neural networks &#x2013; and although I love linear algebra &#x2013; it is only a distraction in a case as simple as this; therefore, the use of explicit notation and the stripping of linear algebra will make the learning curve towards understanding the heart of such a profound machine learning technique a lot less steep. Additionally, the XOR problem is a particularly famous one in neural networks, and I recommend anyone interested to read how XOR helped develop (and almost &#x201C;destroyed&#x201D;) the field of neural networks in its infancy. See &#x201C;Explained: Neural Networks&#x201D; or the 2017 forward to Perceptrons (Minsky &amp; Papert, 1969/1988). 5.2 Activation Since this situation isn&#x2019;t linear (as it is with most natural phenomena), we introduce a non-linearity &#x2013; or activation function &#x2013; into the network to avoid it calculating linear combinations of values as if it were a linear classification task. This is what allows our neural network to successfully begin learning such non-linear phenomena, thus giving it the capability to perform non-linear classification. Figure 5.2: The sigmoid activation function plotted in Mathematica. We use a sigmoidal activation function (Figure 5.2) to squash and map our inputs to the range (0,1)(0,1)(0,1). This function also serves as an abstract representation of the action potential process that takes place during the firing in a neuron: &#x3C6;(x)=11+e&#x2212;x;&#x3C6;(x)&#x2254;&#x3C6;x\\varphi(x) = \\frac{1}{1+e^{-x}}; \\quad \\varphi(x) \\colonequals \\varphi_x&#x3C6;(x)=1+e&#x2212;x1&#x200B;;&#x3C6;(x):=&#x3C6;x&#x200B; which has a neat derivative of: &#x3C6;x(1&#x2212;&#x3C6;x).\\varphi_x (1 - \\varphi_x).&#x3C6;x&#x200B;(1&#x2212;&#x3C6;x&#x200B;). 5.3 Multivariable Calculus The cost function, or error of this network, which we want to minimize is: E=12(y&#x2212;y^)2(1)E = \\frac{1}{2}(y - \\hat y)^2 \\tag{1}E=21&#x200B;(y&#x2212;y^&#x200B;)2(1) where yyy is the correct output and y^\\hat yy^&#x200B; is the network&#x2019;s output. To minimize our error, we have to adjust each of the network&#x2019;s weights until we get the desired output. This is done through backpropagation, using the negative gradient to find the steepest descent towards the local minimum of the error for each weight. Referencing Figure 5.3, this gradient is defined as a vector whose indices are the rate of change in the direction of steepest descent for every weight: &#x2212;&#x2207;E&#x2261;&#x2212;[&#x2202;E&#x2202;w1,&#x2202;E&#x2202;w2,&#x2202;E&#x2202;w11,&#x2026;,&#x2202;E&#x2202;w22](2.1)- \\nabla E \\equiv - \\left[ \\frac{\\partial E}{\\partial w_1}, \\frac{\\partial E}{\\partial w_2}, \\frac{\\partial E}{\\partial w_{11}}, \\ldots , \\frac{\\partial E}{\\partial w_{22}} \\right] \\tag{2.1}&#x2212;&#x2207;E&#x2261;&#x2212;[&#x2202;w1&#x200B;&#x2202;E&#x200B;,&#x2202;w2&#x200B;&#x2202;E&#x200B;,&#x2202;w11&#x200B;&#x2202;E&#x200B;,&#x2026;,&#x2202;w22&#x200B;&#x2202;E&#x200B;](2.1) We use the training rule defined as follows to update all of our weights (defined here generally as www) using the corresponding gradient index which holds that specific weight: &#x394;w=&#x2212;&#x3B1;&#x2202;E&#x2202;w;&#x3B1;&#x2208;(0,1)\\Delta w = - \\alpha \\frac{\\partial E}{\\partial w}; \\quad \\alpha \\in (0,1)&#x394;w=&#x2212;&#x3B1;&#x2202;w&#x2202;E&#x200B;;&#x3B1;&#x2208;(0,1) where &#x3B1;\\alpha&#x3B1; is the learning rate, or how fast the algorithm will try to approach the local minimum. 5.3.1 Dissecting the Network Figure 5.3: XOR Neural Network Diagram that I made in OmniGraffle. With the visual aid above, this entire network can be defined as follows: y^=&#x3C6;(nety^)nety^=&#x3C6;(net1)w1+&#x3C6;(net2)w2net1=i1w11+i2w21net2=i1w12+i2w22\\begin{align*} \\hat y &amp;= \\varphi(\\text{net}_{\\hat y})\\\\ \\text{net}_{\\hat y} &amp;= \\varphi(\\text{net}_1)w_1 + \\varphi(\\text{net}_2)w_2\\\\ \\text{net}_1 &amp;= i_1w_{11} + i_2w_{21}\\\\ \\text{net}_2 &amp;= i_1w_{12}+i_2w_{22} \\end{align*}y^&#x200B;nety^&#x200B;&#x200B;net1&#x200B;net2&#x200B;&#x200B;=&#x3C6;(nety^&#x200B;&#x200B;)=&#x3C6;(net1&#x200B;)w1&#x200B;+&#x3C6;(net2&#x200B;)w2&#x200B;=i1&#x200B;w11&#x200B;+i2&#x200B;w21&#x200B;=i1&#x200B;w12&#x200B;+i2&#x200B;w22&#x200B;&#x200B; This notation will be used when computing the partial derivatives, as to avoid confusion between what is a function and what is multiplication: &#x3C6;(nety^)&#x2254;&#x3C6;y^&#x3C6;(net1)&#x2254;&#x3C6;1&#x3C6;(net2)&#x2254;&#x3C6;2\\begin{align*} \\varphi(\\text{net}_{\\hat y}) \\coloneqq \\varphi_{\\hat y}\\\\ \\varphi(\\text{net}_1) \\coloneqq \\varphi_1\\\\ \\varphi(\\text{net}_2) \\coloneqq \\varphi_2 \\end{align*}&#x3C6;(nety^&#x200B;&#x200B;):=&#x3C6;y^&#x200B;&#x200B;&#x3C6;(net1&#x200B;):=&#x3C6;1&#x200B;&#x3C6;(net2&#x200B;):=&#x3C6;2&#x200B;&#x200B; Since this is a multivariable function, we have to apply the chain rule to take into account everything that&#x2019;s a function of the hidden layer weights: &#x394;wy^=&#x2212;&#x3B1;&#x2202;E&#x2202;wy^=&#x2212;&#x3B1;&#x2202;E&#x2202;y^&#x2202;y^&#x2202;nety^&#x2202;nety^&#x2202;wy^(2.2)\\Delta w_{\\hat y} = - \\alpha \\frac{\\partial E}{\\partial w_{\\hat y}} = - \\alpha \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial \\text{net}_{\\hat y}} \\frac{\\partial \\text{net}_{\\hat y}}{\\partial w_{\\hat y}} \\tag{2.2}&#x394;wy^&#x200B;&#x200B;=&#x2212;&#x3B1;&#x2202;wy^&#x200B;&#x200B;&#x2202;E&#x200B;=&#x2212;&#x3B1;&#x2202;y^&#x200B;&#x2202;E&#x200B;&#x2202;nety^&#x200B;&#x200B;&#x2202;y^&#x200B;&#x200B;&#x2202;wy^&#x200B;&#x200B;&#x2202;nety^&#x200B;&#x200B;&#x200B;(2.2) Equation 2.2 will be used for the proposed adjustments of these two weights, wy^&#x2208;{w1,w2}w_{\\hat y} \\in \\{ w_1, w_2\\}wy^&#x200B;&#x200B;&#x2208;{w1&#x200B;,w2&#x200B;}, which are directly connected to the output node where the partials are: &#x2202;E&#x2202;y^=&#x2202;&#x2202;y^(12(y&#x2212;y^)2)=&#x2212;(y&#x2212;y^)&#x2202;y^&#x2202;nety^=&#x2202;&#x2202;y^(&#x3C6;y^)=&#x3C6;y^(1&#x2212;&#x3C6;y^)&#x2202;nety^&#x2202;wy^=&#x2202;&#x2202;wy^(&#x3C6;1w1+&#x3C6;2w2)=&#x3C6;1&#xA0;&#xA0;or&#xA0;&#xA0;&#x3C6;2&#x2234;&#x394;w1=&#x3B1;(y&#x2212;y^)&#x3C6;y^(1&#x2212;&#x3C6;y^)&#x3C6;1&#x394;w2=&#x3B1;(y&#x2212;y^)&#x3C6;y^(1&#x2212;&#x3C6;y^)&#x3C6;2\\begin{align*}\\\\ \\frac{\\partial E}{\\partial \\hat y} &amp;= \\frac{\\partial}{\\partial \\hat y} \\left(\\frac{1}{2}(y - \\hat y)^2\\right) = -(y - \\hat y)\\\\\\\\ \\frac{\\partial \\hat y}{\\partial \\text{net}_{\\hat y}} &amp;= \\frac{\\partial}{\\partial \\hat y} \\Bigl(\\varphi_{\\hat y}\\Bigr) = \\varphi_{\\hat y}(1-\\varphi_{\\hat y})\\\\\\\\ \\frac{\\partial \\text{net}_{\\hat y}}{\\partial w_{\\hat y}} &amp;= \\frac{\\partial}{\\partial w_{\\hat y}} \\Bigl(\\varphi_1w_1 + \\varphi_2w_2\\Bigr) = \\varphi_1 \\text{ \\ or \\ } \\varphi_2\\\\ \\\\&amp;\\therefore&amp;\\\\\\\\ \\Delta w_1 &amp;= \\alpha(y - \\hat y)\\varphi_{\\hat y}(1-\\varphi_{\\hat y})\\varphi_1 \\\\ \\Delta w_2 &amp;= \\alpha(y - \\hat y)\\varphi_{\\hat y}(1-\\varphi_{\\hat y})\\varphi_2 \\end{align*}&#x2202;y^&#x200B;&#x2202;E&#x200B;&#x2202;nety^&#x200B;&#x200B;&#x2202;y^&#x200B;&#x200B;&#x2202;wy^&#x200B;&#x200B;&#x2202;nety^&#x200B;&#x200B;&#x200B;&#x394;w1&#x200B;&#x394;w2&#x200B;&#x200B;=&#x2202;y^&#x200B;&#x2202;&#x200B;(21&#x200B;(y&#x2212;y^&#x200B;)2)=&#x2212;(y&#x2212;y^&#x200B;)=&#x2202;y^&#x200B;&#x2202;&#x200B;(&#x3C6;y^&#x200B;&#x200B;)=&#x3C6;y^&#x200B;&#x200B;(1&#x2212;&#x3C6;y^&#x200B;&#x200B;)=&#x2202;wy^&#x200B;&#x200B;&#x2202;&#x200B;(&#x3C6;1&#x200B;w1&#x200B;+&#x3C6;2&#x200B;w2&#x200B;)=&#x3C6;1&#x200B;&#xA0;&#xA0;or&#xA0;&#xA0;&#x3C6;2&#x200B;&#x2234;=&#x3B1;(y&#x2212;y^&#x200B;)&#x3C6;y^&#x200B;&#x200B;(1&#x2212;&#x3C6;y^&#x200B;&#x200B;)&#x3C6;1&#x200B;=&#x3B1;(y&#x2212;y^&#x200B;)&#x3C6;y^&#x200B;&#x200B;(1&#x2212;&#x3C6;y^&#x200B;&#x200B;)&#x3C6;2&#x200B;&#x200B;&#x200B; The only noticeable difference between these two weight changes is which net input was activated prior to it. Thus begins our backpropagation! For the four weights directly connected to the input nodes we have to extend the chain rule even further to account for everything that&#x2019;s a function of the inputs: &#x394;wij=&#x2212;&#x3B1;&#x2202;E&#x2202;wij=&#x2212;&#x3B1;&#x2202;E&#x2202;y^&#x2202;y^&#x2202;nety^&#x2202;nety^&#x2202;wy^&#x2202;wy^&#x2202;neti&#x2202;neti&#x2202;wij\\Delta w_{ij} = - \\alpha \\frac{\\partial E}{\\partial w_{ij}} = {\\color{teal} - \\alpha \\frac{\\partial E}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial \\text{net}_{\\hat y}} \\frac{\\partial \\text{net}_{\\hat y}}{\\partial w_{\\hat y}}} \\frac{\\partial w_{\\hat y}}{\\partial \\text{net}_i} \\frac{\\partial \\text{net}_i}{\\partial w_{ij}}&#x394;wij&#x200B;=&#x2212;&#x3B1;&#x2202;wij&#x200B;&#x2202;E&#x200B;=&#x2212;&#x3B1;&#x2202;y^&#x200B;&#x2202;E&#x200B;&#x2202;nety^&#x200B;&#x200B;&#x2202;y^&#x200B;&#x200B;&#x2202;wy^&#x200B;&#x200B;&#x2202;nety^&#x200B;&#x200B;&#x200B;&#x2202;neti&#x200B;&#x2202;wy^&#x200B;&#x200B;&#x200B;&#x2202;wij&#x200B;&#x2202;neti&#x200B;&#x200B; where wijw_{ij}wij&#x200B; represents the jthj^{th}jth weight directly connected to the ithi^{th}ith input. Using Equation 2.2 associated with the above colored text simplifies this to: &#x394;wij=&#x394;wy^&#x2202;wy^&#x2202;neti&#x2202;neti&#x2202;wij(2.3)\\Delta w_{ij} = \\Delta w_{\\hat y} \\frac{\\partial w_{\\hat y}}{\\partial \\text{net}_i} \\frac{\\partial \\text{net}_i}{\\partial w_{ij}} \\tag{2.3}&#x394;wij&#x200B;=&#x394;wy^&#x200B;&#x200B;&#x2202;neti&#x200B;&#x2202;wy^&#x200B;&#x200B;&#x200B;&#x2202;wij&#x200B;&#x2202;neti&#x200B;&#x200B;(2.3) Equation 2.3 will be used for the proposed adjustments of these four weights which are directly connected to the input node, namely wij&#x2208;{w11,w12,w21,w22}w_{ij} \\in \\{w_{11},w_{12},w_{21},w_{22} \\}wij&#x200B;&#x2208;{w11&#x200B;,w12&#x200B;,w21&#x200B;,w22&#x200B;}. Basically, we&#x2019;re looking at each of the four paths that need to be taken in order to reach the original inputs with our starting point being our output (as color-coded in Figure 5.3). Recall that: net1=i1w11+i2w21net2=i1w12+i2w22\\begin{align*} \\text{net}_1 &amp;= i_1w_{11}+i_2w_{21}\\\\ \\text{net}_2 &amp;= i_1w_{12}+i_2w_{22} \\end{align*}net1&#x200B;net2&#x200B;&#x200B;=i1&#x200B;w11&#x200B;+i2&#x200B;w21&#x200B;=i1&#x200B;w12&#x200B;+i2&#x200B;w22&#x200B;&#x200B; Additionally, since the weights connected to the output neuron are influenced by their respective activated net inputs: &#x2202;wy^&#x2202;neti=&#x2202;&#x2202;neti(&#x3C6;i)=&#x3C6;i(1&#x2212;&#x3C6;i)&#x2234;&#x394;w11=&#x394;w1&#xA0;&#x3C6;1(1&#x2212;&#x3C6;1)i1&#x394;w12=&#x394;w2&#xA0;&#x3C6;2(1&#x2212;&#x3C6;2)i1&#x394;w21=&#x394;w1&#xA0;&#x3C6;1(1&#x2212;&#x3C6;1)i2&#x394;w22=&#x394;w2&#xA0;&#x3C6;2(1&#x2212;&#x3C6;2)i2\\begin{align*} \\frac{\\partial w_{\\hat y}}{\\partial \\text{net}_i} &amp;= \\frac{\\partial}{\\partial \\text{net}_i} \\left(\\varphi_i\\right) = \\varphi_i(1 - \\varphi_i)\\\\\\\\ &amp;\\therefore&amp;\\\\\\\\ \\Delta w_{11} &amp;= \\Delta w_1 \\ \\varphi_1(1-\\varphi_1)i_1\\\\ \\Delta w_{12} &amp;= \\Delta w_2 \\ \\varphi_2(1-\\varphi_2)i_1\\\\ \\Delta w_{21} &amp;= \\Delta w_1 \\ \\varphi_1(1-\\varphi_1)i_2\\\\ \\Delta w_{22} &amp;= \\Delta w_2 \\ \\varphi_2(1-\\varphi_2)i_2\\\\ \\end{align*}&#x2202;neti&#x200B;&#x2202;wy^&#x200B;&#x200B;&#x200B;&#x394;w11&#x200B;&#x394;w12&#x200B;&#x394;w21&#x200B;&#x394;w22&#x200B;&#x200B;=&#x2202;neti&#x200B;&#x2202;&#x200B;(&#x3C6;i&#x200B;)=&#x3C6;i&#x200B;(1&#x2212;&#x3C6;i&#x200B;)&#x2234;=&#x394;w1&#x200B;&#xA0;&#x3C6;1&#x200B;(1&#x2212;&#x3C6;1&#x200B;)i1&#x200B;=&#x394;w2&#x200B;&#xA0;&#x3C6;2&#x200B;(1&#x2212;&#x3C6;2&#x200B;)i1&#x200B;=&#x394;w1&#x200B;&#xA0;&#x3C6;1&#x200B;(1&#x2212;&#x3C6;1&#x200B;)i2&#x200B;=&#x394;w2&#x200B;&#xA0;&#x3C6;2&#x200B;(1&#x2212;&#x3C6;2&#x200B;)i2&#x200B;&#x200B; The noticeable differences between these four weight changes are the actual input itself in conjunction with which hidden node the weight&#x2019;s attached to. 5.4 Conclusion Focusing on the XOR case to start, as opposed to tackling the generalized form of neural networks, provides a more easily-digestible intuition for how backpropagation works in neural networks. The network literally propagates backward along every possible path connecting the output node to the input nodes in order to discover the right values for each individual weight. Over multiple gradient descent iterations, this network will successfully converge towards a local minimum of the multivariable error function, therefore learning the XOR gate. This means that you could input any combination of two binary numbers and it would successfully return the correct XOR response to such inputs without looking up a hard-coded truth table. This non-linear classification task was previously impossible for feed-forward networks due to their inherently linear structure. Without the invention of the backpropagation algorithm, neural networks would&#x2019;ve been completely disregarded only a few years after their inception. Within the last decade, the immense strides in neural networks gave birth to a technique called deep learning, now implemented within almost every cutting-edge artificial intelligence system imaginable. 5.5 Learning XOR in TensorFlow The truth table for XOR is as follows: ABA&#x2295;B000011101110\\begin{array}{cc|c} A &amp; B &amp; A \\oplus B\\\\ \\hline 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 1\\\\ 1 &amp; 0 &amp; 1\\\\ 1 &amp; 1 &amp; 0\\\\ \\end{array}A0011&#x200B;B0101&#x200B;A&#x2295;B0110&#x200B;&#x200B; The model adjusts its weights and converges to the truth table over the training iterations, as displayed in the output below. # Neural Network (NN) in TensorFlow that learns the XOR gate # Edit 2022: load older tf for old code import tensorflow.compat.v1 as tf tf.disable_v2_behavior() # Sigmoid activation function ## WARNING:tensorflow:From /usr/local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version. ## Instructions for updating: ## non-resource variables are not supported in the long term def activate(nodes, weights): net = tf.matmul(nodes, weights) return tf.nn.sigmoid(net) # XOR truth table X_XOR = [[0,0],[0,1],[1,0],[1,1]] Y_XOR = [[0],[1],[1],[0]] # NN parameters N_STEPS = 250000 HIDDEN_NODES = 2 INPUT_NODES = 2 OUTPUT_NODES = 1 LEARNING_RATE = 0.05 # NN I/O x_nn = tf.placeholder(tf.float32, shape=[len(X_XOR), INPUT_NODES]) y_nn = tf.placeholder(tf.float32, shape=[len(X_XOR), OUTPUT_NODES]) # Randomized NN weights for input layer and hidden layer w_i = tf.Variable(tf.random_uniform([INPUT_NODES, HIDDEN_NODES], 0, 1)) w_h = tf.Variable(tf.random_uniform([HIDDEN_NODES, OUTPUT_NODES], 0, 1)) # Forward Pass through the layers hidden = activate(x_nn, w_i) output = activate(hidden, w_h) # Cost function (MSE) and Backpropagation cost = tf.reduce_mean(tf.square(Y_XOR - output)) backprop = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cost) # Feed dictionary of truth table dict_xor = {x_nn: X_XOR, y_nn: Y_XOR} init = tf.global_variables_initializer() sess = tf.Session() sess.run(init) for i in range(N_STEPS+1): sess.run(backprop, feed_dict=dict_xor) if i in [0, 25000, 50000, 100000, 250000]: print(&quot;--------------------&quot;) print(&quot;Iteration&quot;, i, &quot;Guess:&quot;, sess.run(tf.transpose(output), feed_dict=dict_xor), &quot;\\n&quot;) print(&quot;Input Weights:\\n&quot;, sess.run(w_i)) print(&quot;Hidden Weights:\\n&quot;, sess.run(w_h), &quot;\\n&quot;) print(&quot;Error:&quot;, sess.run(cost, feed_dict=dict_xor)) ## -------------------- ## Iteration 0 Guess: [[0.6382959 0.67482334 0.68818045 0.712769 ]] ## ## Input Weights: ## [[0.78813964 0.86214846] ## [0.13670845 0.91594166]] ## Hidden Weights: ## [[0.45105302] ## [0.68489796]] ## ## Error: 0.27960813 ## -------------------- ## Iteration 25000 Guess: [[0.2687148 0.68361616 0.68345094 0.408158 ]] ## ## Input Weights: ## [[0.7330665 4.97169 ] ## [0.73106855 4.907222 ]] ## Hidden Weights: ## [[-8.683456 ] ## [ 6.6811504]] ## ## Error: 0.109775655 ## -------------------- ## Iteration 50000 Guess: [[0.15894006 0.7772046 0.77719414 0.29180697]] ## ## Input Weights: ## [[0.8214333 6.0759916] ## [0.8212353 6.054871 ]] ## Hidden Weights: ## [[-15.087451] ## [ 11.755179]] ## ## Error: 0.052423377 ## -------------------- ## Iteration 100000 Guess: [[0.09161668 0.8497939 0.8497923 0.19840614]] ## ## Input Weights: ## [[0.87447494 6.8183227 ] ## [0.8744301 6.8081775 ]] ## Hidden Weights: ## [[-21.539515] ## [ 16.951408]] ## ## Error: 0.023220709 ## -------------------- ## Iteration 250000 Guess: [[0.04534886 0.9112044 0.9112041 0.11809696]] ## ## Input Weights: ## [[0.91362447 7.478241 ] ## [0.9136126 7.4730563 ]] ## Hidden Weights: ## [[-29.468285] ## [ 23.374363]] ## ## Error: 0.007943195 "],["nn-mnist.html", "Chapter 6 NN: MNIST [ML|R,MATH] 6.1 Setup &amp; Importing Data 6.2 Algorithm", " Chapter 6 NN: MNIST [ML|R,MATH] Under the Hood: Neural Networks and Backpropagation I made a vanilla neural network from scratch in R to showcase its inner workings. Specifically, this is a multilayer perceptron (MLP) &#x2013; a fully connected class of feedforward artificial neural network (ANN). An ANN equipped with backpropagation can learn non-linear classification tasks. This is an &#x201C;unofficial sequel&#x201D; to my XOR project and demonstrates the mathematics behind backpropagation by learning how to classify handwritten digits. I used the MNIST handwritten digits dataset. 6.1 Setup &amp; Importing Data First, I created a shell script to scrape and decompress the data so I can feed it into R: #!/bin/zsh wget https://data.deepai.org/mnist.zip \\ &amp;&amp; unzip &apos;*.zip&apos; -d mnist-data/ \\ &amp;&amp; rm *.zip &amp;&amp; rm -rf __MACOSX/ \\ &amp;&amp; cd mnist-data/ &amp;&amp; gzip -d *.gz Then created helper functions to parse the data from byte-form: # Returns a list of matrices containing image gray-scale values read_imgs &lt;- function(imgdb, n_get=0) { images &lt;- c() readBin(imgdb, integer(), n=1, endian=&quot;big&quot;) n_imgs &lt;- readBin(imgdb, integer(), n=1, endian=&quot;big&quot;) if(n_get==0) n_get &lt;- n_imgs nrows &lt;- readBin(imgdb, integer(), n=1, endian=&quot;big&quot;) ncols &lt;- readBin(imgdb, integer(), n=1, endian=&quot;big&quot;) for(i in 1:n_get) { img &lt;- matrix(readBin(imgdb, integer(), n=nrows*ncols, size=1, signed=FALSE), nrows, ncols) images &lt;- c(images, list(img)) } close(imgdb) return(images) } # Returns a list of image labels read_lbls &lt;- function(lbldb, n_get=0) { readBin(lbldb, integer(), n=1, endian=&quot;big&quot;) n_lbls &lt;- readBin(lbldb, integer(), n=1, endian=&quot;big&quot;) if(n_get == 0) n_get = n_lbls lbls &lt;- readBin(lbldb, integer(), n=n_get, size=1, signed=FALSE) return(lbls) } 6.1.1 Data # Get images mnist_imgs &lt;- file(&quot;mnist-data/train-images-idx3-ubyte&quot;, &quot;rb&quot;) imgs &lt;- read_imgs(mnist_imgs, 9) # Get labels mnist_lbls &lt;- file(&quot;mnist-data/train-labels-idx1-ubyte&quot;, &quot;rb&quot;) lbls &lt;- read_lbls(mnist_lbls, 9) # Display example digit with label image(imgs[[1]][,28:1], col=gray(12:1/12), axes = FALSE, main=paste(lbls[1])) Figure 6.1: The first labeled digit in the MNIST dataset # 9 labeled digits par(mfrow=c(3,3)) par(mar=c(0, 0, 3, 0)) for(i in 1:9){ image(imgs[[i]][,28:1], col=gray(12:1/12), axes=FALSE, main=paste(lbls[i])) } # Number of pixels length(imgs[[1]]) ## [1] 784 6.2 Algorithm The 10 output nodes represent the digits &#xA0;0&#x2212;9\\ 0-9&#xA0;0&#x2212;9 Since the images are 28 by 28 we have 282=78428^2 = 784282=784 pixels, or 784 input nodes The number of hidden nodes determines the model&#x2019;s complexity; i.e., underfitting or overfitting Can be tuned; for this example, I chose 28 (elegance and it saves some computation) This is a showcase of the &#x201C;heart&#x201D; of the neural network; in practice, this algorithm would be running in batches with weights updated over epochs. To not take away from the mathematics (and for brevity&#x2019;s sake) the algorithm only trains on one example. 6.2.1 Initialize This example focuses on the first digit from Figure 6.1, providing a walkthrough of how the learning process works for classifying a handwritten digit. set.seed(1) # Sigmoid activate &lt;- function(node) { return(matrix(1/(1+exp(-node)))) } # Derivative sigprime &lt;- function(node) { return(matrix(activate(node)*(1 - activate(node)))) } model &lt;- list() errs &lt;- list() # Learning rate alpha &lt;- 0.25 # Number of input, hidden, output nodes network &lt;- c(784, 28, 10) # Using the first digit number 5 for example model$input &lt;- matrix(imgs[[1]]) # Since the first label is 5, it&apos;s the 6th index of our truth vector truth &lt;- matrix(rep(0,10), ncol=1) truth[lbls[1]+1] &lt;- 1 truth ## [,1] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 0 ## [6,] 1 ## [7,] 0 ## [8,] 0 ## [9,] 0 ## [10,] 0 # Initialize nodes model$nodes &lt;- mapply(matrix, data=0, ncol=1, nrow=network) lengths(model$nodes) ## [1] 784 28 10 # Initialize random weights model$weights &lt;- lapply(1:(length(network)-1), function(k) { matrix(rnorm(network[k+1]*network[k]), nrow=network[k+1], ncol=network[k]) }) lengths(model$weights) ## [1] 21952 280 # Initialize random biases b &lt;- numeric() b &lt;- lapply(network[-1], rnorm) model$biases &lt;- mapply(matrix, data=b, ncol=1, nrow=network[-1]) lengths(model$biases) ## [1] 28 10 6.2.2 Learn For each iteration, the training example undergoes one forward pass and one backward pass. During the backward pass, the weights are updated according to the gradient of the quadratic loss function &#x2013; as derived in the XOR project. # Iterations N = 250 for(i in 1:N) { # Feed Forward model$nodes[[1]] &lt;- matrix(model$input) # Activate hidden layer model$nodes[[2]] &lt;- model$weights[[1]]%*%model$nodes[[1]] + model$biases[[1]] model$active[[1]] &lt;- activate(model$nodes[[2]]) # Activate output layer model$nodes[[3]] &lt;- model$weights[[2]]%*%model$active[[1]] + model$biases[[2]] model$active[[2]] &lt;- activate(model$nodes[[3]]) # Backpropagation errs[[i]] &lt;- model$active[[2]] - truth delta_w &lt;- list() delta_w[[2]] &lt;- alpha * (truth - model$active[[2]]) * sigprime(model$nodes[[3]]) delta_w[[1]] &lt;- (t(model$weights[[2]])%*%delta_w[[2]]) * sigprime(model$nodes[[2]]) # Update weights w2 &lt;- model$weights[[2]] + delta_w[[2]]%*%t(model$active[[1]]) w1 &lt;- model$weights[[1]] + delta_w[[1]]%*%t(model$nodes[[1]]) model$weights[[2]] &lt;- w2 model$weights[[1]] &lt;- w1 # Update biases b2 &lt;- model$biases[[2]] - alpha*delta_w[[2]] b1 &lt;- model$biases[[1]] - alpha*delta_w[[1]] model$biases[[2]] &lt;- b2 model$biases[[1]] &lt;- b1 # Print results if(i %% 10 == 0 &amp;&amp; i &lt; 70){ print(&quot;--------------------&quot;) print(paste(&quot;Iteration&quot;, i, &quot;Guess:&quot;, which.max(as.vector(model$active[[2]])) - 1)) print(matrix(model$active[[2]])) } else if(i == N){ print(&quot;--------------------&quot;) print(paste(&quot;Iteration&quot;, i, &quot;Guess:&quot;, which.max(as.vector(model$active[[2]])) - 1)) print(matrix(model$active[[2]])) } } ## [1] &quot;--------------------&quot; ## [1] &quot;Iteration 10 Guess: 4&quot; ## [,1] ## [1,] 0.1616835267 ## [2,] 0.1264294918 ## [3,] 0.0682142668 ## [4,] 0.2038745379 ## [5,] 0.9941272877 ## [6,] 0.0569540176 ## [7,] 0.0008336876 ## [8,] 0.9663277915 ## [9,] 0.0124175449 ## [10,] 0.0368053450 ## [1] &quot;--------------------&quot; ## [1] &quot;Iteration 20 Guess: 4&quot; ## [,1] ## [1,] 0.1014268525 ## [2,] 0.0899760227 ## [3,] 0.0597189587 ## [4,] 0.1110630825 ## [5,] 0.9925675618 ## [6,] 0.6975266674 ## [7,] 0.0008336663 ## [8,] 0.7302379058 ## [9,] 0.0123492076 ## [10,] 0.0352006765 ## [1] &quot;--------------------&quot; ## [1] &quot;Iteration 30 Guess: 4&quot; ## [,1] ## [1,] 0.079083747 ## [2,] 0.073107411 ## [3,] 0.053704279 ## [4,] 0.083607664 ## [5,] 0.989914254 ## [6,] 0.875496648 ## [7,] 0.000833645 ## [8,] 0.155205187 ## [9,] 0.012281978 ## [10,] 0.033785417 ## [1] &quot;--------------------&quot; ## [1] &quot;Iteration 40 Guess: 4&quot; ## [,1] ## [1,] 0.0667571496 ## [2,] 0.0629770018 ## [3,] 0.0491670468 ## [4,] 0.0694771807 ## [5,] 0.9844647273 ## [6,] 0.9107698051 ## [7,] 0.0008336237 ## [8,] 0.0996000976 ## [9,] 0.0122158252 ## [10,] 0.0325252556 ## [1] &quot;--------------------&quot; ## [1] &quot;Iteration 50 Guess: 4&quot; ## [,1] ## [1,] 0.0587198417 ## [2,] 0.0560652580 ## [3,] 0.0455913634 ## [4,] 0.0605741282 ## [5,] 0.9677833352 ## [6,] 0.9273060284 ## [7,] 0.0008336023 ## [8,] 0.0781756562 ## [9,] 0.0121507221 ## [10,] 0.0313939679 ## [1] &quot;--------------------&quot; ## [1] &quot;Iteration 60 Guess: 5&quot; ## [,1] ## [1,] 0.052966646 ## [2,] 0.050975717 ## [3,] 0.042681855 ## [4,] 0.054330558 ## [5,] 0.768476767 ## [6,] 0.937292543 ## [7,] 0.000833581 ## [8,] 0.066196623 ## [9,] 0.012086641 ## [10,] 0.030371123 ## [1] &quot;--------------------&quot; ## [1] &quot;Iteration 250 Guess: 5&quot; ## [,1] ## [1,] 0.0244670572 ## [2,] 0.0242512556 ## [3,] 0.0231158041 ## [4,] 0.0246047195 ## [5,] 0.0282727666 ## [6,] 0.9746982789 ## [7,] 0.0008331762 ## [8,] 0.0255317191 ## [9,] 0.0110330649 ## [10,] 0.0203249132 Recall that since R indexes by 1 we need to subtract 1 from which.max() to get the actual number. By iteration 60 it switched its answer to 5 but still held onto 4 as a somewhat close second. We notice however by the 250th iteration, on its noble quest to clear the remnants of its error, it&#x2019;s certain the correct answer&#x2019;s 5. for(i in 1:length(errs)){ if(i %% 10 == 0 &amp;&amp; (i &lt; 70 || i == N)){ print(paste(&quot;Errors -- Iteration&quot;, i)) print(paste(errs[[i]])) } } ## [1] &quot;Errors -- Iteration 10&quot; ## [1] &quot;0.161683526706662&quot; &quot;0.126429491773764&quot; &quot;0.0682142668115703&quot; ## [4] &quot;0.203874537919715&quot; &quot;0.994127287704014&quot; &quot;-0.943045982396918&quot; ## [7] &quot;0.000833687636843194&quot; &quot;0.966327791499464&quot; &quot;0.0124175448799003&quot; ## [10] &quot;0.0368053450002197&quot; ## [1] &quot;Errors -- Iteration 20&quot; ## [1] &quot;0.101426852454415&quot; &quot;0.0899760227150036&quot; &quot;0.0597189587251529&quot; ## [4] &quot;0.11106308253193&quot; &quot;0.992567561771656&quot; &quot;-0.302473332555646&quot; ## [7] &quot;0.00083366630628941&quot; &quot;0.730237905751141&quot; &quot;0.0123492076029388&quot; ## [10] &quot;0.0352006764929306&quot; ## [1] &quot;Errors -- Iteration 30&quot; ## [1] &quot;0.0790837469870936&quot; &quot;0.073107410627621&quot; &quot;0.0537042786567521&quot; ## [4] &quot;0.0836076637326415&quot; &quot;0.989914254344727&quot; &quot;-0.124503351562107&quot; ## [7] &quot;0.000833644977371914&quot; &quot;0.155205186699078&quot; &quot;0.0122819775282261&quot; ## [10] &quot;0.0337854167653882&quot; ## [1] &quot;Errors -- Iteration 40&quot; ## [1] &quot;0.0667571495768358&quot; &quot;0.0629770017850248&quot; &quot;0.0491670467964501&quot; ## [4] &quot;0.0694771807081971&quot; &quot;0.984464727283454&quot; &quot;-0.0892301948984062&quot; ## [7] &quot;0.000833623650090492&quot; &quot;0.0996000976300502&quot; &quot;0.0122158251682742&quot; ## [10] &quot;0.0325252556194334&quot; ## [1] &quot;Errors -- Iteration 50&quot; ## [1] &quot;0.0587198417202392&quot; &quot;0.0560652580072607&quot; &quot;0.0455913634312125&quot; ## [4] &quot;0.0605741281916175&quot; &quot;0.96778333516366&quot; &quot;-0.072693971563165&quot; ## [7] &quot;0.000833602324444938&quot; &quot;0.0781756561618741&quot; &quot;0.0121507221215724&quot; ## [10] &quot;0.0313939678634957&quot; ## [1] &quot;Errors -- Iteration 60&quot; ## [1] &quot;0.0529666457980008&quot; &quot;0.0509757169601339&quot; &quot;0.0426818547984694&quot; ## [4] &quot;0.0543305581185968&quot; &quot;0.768476767077998&quot; &quot;-0.062707456643689&quot; ## [7] &quot;0.000833581000435041&quot; &quot;0.0661966226652927&quot; &quot;0.0120866410217594&quot; ## [10] &quot;0.0303711231306996&quot; ## [1] &quot;Errors -- Iteration 250&quot; ## [1] &quot;0.0244670571849342&quot; &quot;0.024251255591348&quot; &quot;0.0231158041089027&quot; ## [4] &quot;0.0246047194786425&quot; &quot;0.0282727666393937&quot; &quot;-0.0253017210800204&quot; ## [7] &quot;0.00083317615474425&quot; &quot;0.025531719110534&quot; &quot;0.011033064925959&quot; ## [10] &quot;0.0203249132299526&quot; Aside &#x201C;Humanizing&#x201D; the algorithm as such helps shine a light on what the hidden features could represent in this kind of modeling: The training example (Figure 6.1) does kind of look like a 4 rotated 90 degrees clockwise &#x2013; so it&#x2019;s understandable why the model was confused for a while. It&#x2019;s also curious how in the early iterations it has 7 as a close second &#x2013; this 5 is written in a jagged form with edges like a 7, so it also makes sense. Additionally, recall that this model trained on one 5. The more training examples of 5s it&#x2019;s fed, the better it&#x2019;ll get at classifying different variations of handwritten 5s. Generally speaking, this model would be ran on all of the handwritten digits in the training set (0-9). Subsequently, the weights are adjusted in such a way that allows the model to take any of the aforementioned digits as input and properly classify which digit it is. Therefore, the more instances of each digit in the training set, the better the model is at classifying each digit and its respective handwritten variations. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
